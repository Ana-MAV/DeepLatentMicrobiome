{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "banner-september",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid de la segunda parte, en la que utilizamos ya la primera parte bien\n",
    "#Esta va a ser la de las 44 variables con combined\n",
    "\n",
    "\n",
    "#importamos paquetes\n",
    "import sys\n",
    "sys.path.append('Src/')\n",
    "from data_modificado import * #hay funciones que estan cambiadas en este script para adaptralas a nuestro dataset\n",
    "from train_2 import * #este hubo que modificar una linea tambien\n",
    "from transfer_learning import * #hubo que modificart lo mismo que en train_2\n",
    "from test_functions import *\n",
    "from layers import *\n",
    "from utils import *\n",
    "from loss import *\n",
    "from metric import *\n",
    "from results import *\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "import itertools as it\n",
    "\n",
    "#funciones\n",
    "def read_df(\n",
    "              metadata_names=['age','Temperature','Precipitation3Days'],\n",
    "              random_state=42,\n",
    "              otu_filename='../Datasets/otu_table_all_80.csv',\n",
    "              metadata_filename='../Datasets/metadata_table_all_80.csv'):\n",
    "    otu = pd.read_csv(otu_filename, index_col=0, header=None).T\n",
    "    #print(otu.head())\n",
    "    otu = otu.set_index('otuids')\n",
    "    otu = otu.astype('int32')\n",
    "    metadata = pd.read_csv(metadata_filename)\n",
    "    #print(metadata.head())\n",
    "    metadata = metadata.set_index('X.SampleID')\n",
    "    metadata.head()\n",
    "    domain = metadata[metadata_names]\n",
    "    #if 'INBREDS' in metadata_names:\n",
    "    #    domain = pd.concat([domain, pd.get_dummies(domain['INBREDS'], prefix='INBREDS')], axis=1)\n",
    "    #    domain = domain.drop(['INBREDS'], axis=1)\n",
    "    #elif 'Maize_Line' in metadata_names:\n",
    "    #    domain = pd.concat([domain, pd.get_dummies(domain['Maize_Line'], prefix='Maize_Line')], axis=1)\n",
    "    #    domain = domain.drop(['Maize_Line'], axis=1) \n",
    "    df = pd.concat([otu, domain], axis=1, sort=True, join='outer')\n",
    "    #print(df.head())\n",
    "    #data_microbioma = df[otu.columns].to_numpy(dtype=np.float32)\n",
    "    #data_domain = df[domain.columns].to_numpy(dtype=np.float32)\n",
    "    df_microbioma = df[otu.columns]\n",
    "    df_domain = df[domain.columns]\n",
    "    df_domain.head()\n",
    "    df_microbioma_train, df_microbioma_no_train, df_domain_train, df_domain_no_train = \\\n",
    "        train_test_split(df_microbioma, df_domain, test_size=0.1, random_state=random_state)\n",
    "    # Transfer learning subset\n",
    "    df_microbioma_test, df_microbioma_transfer_learning, df_domain_test, df_domain_transfer_learning = \\\n",
    "        train_test_split(df_microbioma_no_train, df_domain_no_train, test_size=0.1, random_state=random_state)\n",
    "    df_microbioma_transfer_learning_train, df_microbioma_transfer_learning_test, df_domain_transfer_learning_train, df_domain_transfer_learning_test = \\\n",
    "        train_test_split(df_microbioma_transfer_learning, df_domain_transfer_learning, test_size=0.3, random_state=random_state)\n",
    "    \n",
    "    return df_microbioma_train, df_microbioma_test, df_microbioma_transfer_learning_train, df_microbioma_transfer_learning_test, df_domain_train, df_domain_test, df_domain_transfer_learning_train, df_domain_transfer_learning_test, otu.columns, domain.columns\n",
    "    #return df_microbioma_train, df_microbioma_test, df_microbioma_transfer_learning_train, df_microbioma_transfer_learning_test, df_domain_train, df_domain_test, df_domain_transfer_learning_train, df_domain_transfer_learning_test, otu.columns, domain.columns\n",
    "\n",
    "\n",
    "def train_kfold_mod(model_fn, m_train, d_train, z_train, m_test, d_test, z_test,\n",
    "                batch_size, epochs, train_callbacks):\n",
    "    all_models = model_fn()\n",
    "    model, encoder_bioma, encoder_domain, decoder_bioma = all_models\n",
    "    metrics_prefix = None\n",
    "    if encoder_bioma is not None and encoder_domain is not None:\n",
    "        x_train = (m_train, d_train)\n",
    "        y_train = (m_train, m_train, z_train)\n",
    "        x_test = (m_test, d_test)\n",
    "        y_test = (m_test, m_test, z_test)\n",
    "    elif encoder_bioma is not None:\n",
    "        x_train = m_train\n",
    "        y_train = m_train\n",
    "        x_test = m_test\n",
    "        y_test = m_test\n",
    "        metrics_prefix = 'bioma'\n",
    "    elif encoder_domain is not None:\n",
    "        x_train = d_train\n",
    "        y_train = m_train\n",
    "        x_test = d_test\n",
    "        y_test = m_test\n",
    "        metrics_prefix = 'domain'\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(5000).batch(\n",
    "        batch_size)\n",
    "    train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "    val_dataset = val_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    r = model.fit(train_dataset,\n",
    "                  epochs=epochs,\n",
    "                  validation_data=val_dataset,\n",
    "                  callbacks=train_callbacks,\n",
    "                  verbose=0)\n",
    "    if metrics_prefix is not None:\n",
    "        old_keys = r.history\n",
    "        r.history = {}\n",
    "        for k, v in old_keys.items():\n",
    "            if k == 'loss' or k == 'val_loss':\n",
    "                new_key = k\n",
    "            elif k.startswith('val_'):\n",
    "                new_key = 'val_{}_{}'.format(metrics_prefix, k[4:])\n",
    "            else:\n",
    "                new_key = '{}_{}'.format(metrics_prefix, k)\n",
    "            r.history[new_key] = v\n",
    "    del val_dataset\n",
    "    del train_dataset\n",
    "    del x_train\n",
    "    del y_train\n",
    "    del x_test\n",
    "    del y_test\n",
    "    return r, all_models\n",
    "\n",
    "def train_2(model_fn,\n",
    "          data_microbioma,\n",
    "          data_domain,\n",
    "          latent_space=10,\n",
    "          folds=5,\n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          learning_rate_scheduler=ExpDecayScheluder(),\n",
    "          random_seed=347,\n",
    "          verbose=0):\n",
    "    data_zeros_latent = np.zeros((data_microbioma.shape[0], latent_space), dtype=data_microbioma.dtype)\n",
    "    results = []\n",
    "    models = []\n",
    "    train_callbacks = [\n",
    "        callbacks.EarlyStopping(monitor='val_loss', patience=epochs + 1, restore_best_weights=True)]\n",
    "    if verbose >= 0:\n",
    "        train_callbacks += [TqdmCallback(verbose=verbose)]\n",
    "    if learning_rate_scheduler is not None:\n",
    "        train_callbacks += [learning_rate_scheduler.make()]\n",
    "\n",
    "    if folds <= 1:\n",
    "        m_train, m_test = data_microbioma, data_microbioma\n",
    "        d_train, d_test = data_domain, data_domain\n",
    "        z_train, z_test = data_zeros_latent, data_zeros_latent\n",
    "        tf.random.set_seed(random_seed)\n",
    "        r, m = train_kfold(model_fn, m_train, d_train, z_train, m_test, d_test, z_test,\n",
    "                           batch_size, epochs, train_callbacks)\n",
    "        results.append(r)\n",
    "        models.append(m)\n",
    "\n",
    "    else: #EL PROBLEMA ESTA AQUI, QUE HACE FALTA UN \n",
    "        kf = KFold(n_splits=folds, random_state=random_seed, shuffle=True)\n",
    "        tf.random.set_seed(random_seed)\n",
    "\n",
    "        for train_index, test_index in kf.split(data_microbioma):\n",
    "            m_train, m_test = data_microbioma[train_index], data_microbioma[test_index]\n",
    "            #print(m_train)\n",
    "            #d_train, d_test = data_domain[train_index], data_domain[test_index]\n",
    "            if data_domain is None:\n",
    "                d_train, d_test = None, None\n",
    "            else:\n",
    "                d_train, d_test = data_domain[train_index], data_domain[test_index]\n",
    "            #print(d_train)\n",
    "            #Esto de hacer el if else ha funcionado, pero no se si hace lo que debe bien\n",
    "            z_train, z_test = data_zeros_latent[train_index], data_zeros_latent[test_index]\n",
    "            r, m = train_kfold_mod(model_fn, m_train, d_train, z_train, m_test, d_test, z_test,\n",
    "                               batch_size, epochs, train_callbacks)\n",
    "            results.append(r)\n",
    "            models.append(m)\n",
    "    return results, models\n",
    "\n",
    "def perform_experiment_2_mod(cv_folds, epochs, batch_size, learning_rate, optimizer,\n",
    "                       learning_rate_scheduler, input_transform, output_transform,\n",
    "                       reconstruction_loss, latent_space, layers,\n",
    "                       activation, activation_latent,\n",
    "                       data_microbioma_train, data_domain_train,\n",
    "                       show_results=True, device='/CPU:0'): #Show results cambiado de False  aTrue\n",
    "    if input_transform is not None:\n",
    "        input_transform = input_transform()\n",
    "    #----------    \n",
    "    if output_transform is not None:\n",
    "        output_transform = output_transform()\n",
    "    #----------      \n",
    "    if reconstruction_loss.__class__.__name__ == 'MakeLoss':\n",
    "        reconstruction_loss = reconstruction_loss.make()\n",
    "    else:\n",
    "        reconstruction_loss = reconstruction_loss()\n",
    "    domain_layers = [l // 16 for l in layers] ####que es esto???? Esto es para las capas del domain\n",
    "    #print(domain_layers)\n",
    "    bioma_autoencoder = \" -> \".join([\"b\"] +\n",
    "                                    [str(l) for l in layers] +\n",
    "                                    [str(latent_space)] +\n",
    "                                    [str(l) for l in reversed(layers)] +\n",
    "                                    [\"b\"])\n",
    "    #---------- \n",
    "    #esto solo se utiliza para el texto, es irrelevante para nuestro error\n",
    "    if data_domain_train is not None:\n",
    "        domain_autoencoder = \" -> \".join([\"d\"] +\n",
    "                                     [str(l) for l in domain_layers] +\n",
    "                                     [str(latent_space)] +\n",
    "                                     [str(l) for l in reversed(layers)] +\n",
    "                                     [\"b\"])\n",
    "        \n",
    "    else: \n",
    "        domain_autoencoder = \" \"\n",
    "    #---------- \n",
    "    #donde se usa domain autoencoder?\n",
    "    in_transform_name = input_transform.__class__.__name__ if input_transform else \"none\"\n",
    "    out_transform_name = output_transform.__class__.__name__ if output_transform else \"none\"\n",
    "    lr_scheduler_text = learning_rate_scheduler[\n",
    "        1] if learning_rate_scheduler is not None else \"none\"\n",
    "    lr_text = learning_rate if learning_rate_scheduler is not None else \"constant = {}\".format(\n",
    "        learning_rate)\n",
    "    learning_rate_scheduler = learning_rate_scheduler[\n",
    "        0] if learning_rate_scheduler is not None else None\n",
    "    optimizer = optimizer(learning_rate=learning_rate)\n",
    "    #---------- \n",
    "    experiment_parameters = [\n",
    "        (\"Input transform\", in_transform_name),\n",
    "        (\"Output transform\", out_transform_name),\n",
    "        (\"Reconstruction Loss\", reconstruction_loss.__class__.__name__),\n",
    "        (\"Latent Space\", latent_space),\n",
    "        (\"Bioma Autoencoder\", bioma_autoencoder),\n",
    "        (\"Domain Autoencoder\", domain_autoencoder),\n",
    "        (\"Activation Encoder\", activation),\n",
    "        (\"Activation Decoder\", activation),\n",
    "        (\"Activation Latent\", activation_latent),\n",
    "        (\"CV folds\", cv_folds),\n",
    "        (\"Epochs\", epochs),\n",
    "        (\"Batch Size\", batch_size),\n",
    "        (\"Learning Rate Scheduler\", lr_scheduler_text),\n",
    "        (\"Learning Rate\", lr_text),\n",
    "        (\"Optimizer\", optimizer.__class__.__name__),\n",
    "    ]\n",
    "    #----------  \n",
    "    if show_results:\n",
    "        md_text = \"\"\n",
    "        md_text += \"| Parameter             | Value         |\\n\"\n",
    "        md_text += \"|:----------------------|:--------------|\\n\"\n",
    "        for n, v in experiment_parameters:\n",
    "            md_text += \"| {} | {} |\\n\".format(n, v)\n",
    "\n",
    "        display(Markdown(md_text))\n",
    "    #------------\n",
    "    def create_model(print_data=False):\n",
    "        bioma_shape=data_microbioma_train.shape[1]\n",
    "        \n",
    "        if data_domain_train is not None:\n",
    "            domain_shape=data_domain_train.shape[1]\n",
    "            #print(\"data_domain_train!=None\")\n",
    "        else:\n",
    "            domain_shape=None\n",
    "            #print(\"data_domain_train==None\")\n",
    "        models = autoencoder(bioma_shape=bioma_shape,\n",
    "                             #bioma_shape=717,\n",
    "                             domain_shape=domain_shape,\n",
    "                             output_shape=bioma_shape,\n",
    "                             #output_shape=717,\n",
    "                             latent_space=latent_space,\n",
    "                             bioma_layers=layers, #Esto es lo de [512,316]\n",
    "                             domain_layers=domain_layers, #Esto son cada una de las layers divididas por 16\n",
    "                             input_transform=input_transform,\n",
    "                             output_transform=output_transform,\n",
    "                             activation_function_encoder=activation,\n",
    "                             activation_function_decoder=activation,\n",
    "                             activation_function_latent=activation_latent)\n",
    "        #Entiendo analizando lo demas que aqui NO esta el error\n",
    "        #la funcion autoencoder esta en model.py (es la unica funcion en ese script)\n",
    "        \n",
    "        model, encoder_bioma, encoder_domain, decoder_bioma = models\n",
    "\n",
    "        if print_data:\n",
    "            plot_models(model, encoder_bioma, encoder_domain, decoder_bioma)\n",
    "        compile_train(model,\n",
    "                      encoder_bioma=encoder_bioma,\n",
    "                      encoder_domain=encoder_domain,\n",
    "                      reconstruction_error=reconstruction_loss,\n",
    "                      encoded_comparison_error=losses.MeanAbsoluteError(),\n",
    "                      metrics=get_experiment_metrics(input_transform, output_transform),\n",
    "                      optimizer=optimizer)\n",
    "        \n",
    "        #print(\"He acabado create_model :)\")\n",
    "        return model, encoder_bioma, encoder_domain, decoder_bioma\n",
    "    #-----------\n",
    "    create_model(print_data=False)\n",
    "    #-----------\n",
    "    #Esta en esta seccion el problema, en train_2\n",
    "    #print(data_domain_train)\n",
    "    #print(latent_space)\n",
    "    with tf.device(device):\n",
    "        results, models = train_2(create_model,\n",
    "                                data_microbioma_train,\n",
    "                                data_domain_train,\n",
    "                                latent_space=latent_space,\n",
    "                                folds=cv_folds,\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                learning_rate_scheduler=learning_rate_scheduler,\n",
    "                                verbose=-1)\n",
    "    #----------\n",
    "    validation_results = print_results(results, show_results=show_results)\n",
    "    if show_results:\n",
    "        display(Markdown(\"*************\"))\n",
    "\n",
    "    return experiment_parameters + validation_results, models, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-timer",
   "metadata": {},
   "source": [
    "__params:__\n",
    " - \"activat_func\":[\"softmax\",\"sigmoid\",\"relu\",\"tanh\"]\n",
    " - \"activ_ouput\":[\"softmax\",\"sigmoid\",\"relu\",\"tanh\"]\n",
    " - \"learning_rate\":[0.01,0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "optimum-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos los datos\n",
    "#nombres_metadatos = [\"KCAL\",\"PROT\",\"TFAT\",\"CARB\",\"MOIS\",\"ALC\",\"CAFF\",\"THEO\",\"SUGR\",\"FIBE\",\"CALC\",\"IRON\",\"MAGN\",\"PHOS\",\"POTA\",\"SODI\",\"ZINC\",\"COPP\",\"SELE\",\"VC\",\"VB1\",\"VB2\",\"NIAC\",\"VB6\",\"FOLA\",\"VB12\",\"VARA\",\"RET\",\"BCAR\",\"ACAR\",\"CRYP\",\"LYCO\",\"LZ\",\"ATOC\",\"VK\",\"CHOLE\",\"SFAT\",\"MFAT\",\"PFAT\",\"VITD\",\"CHOLN\"]\n",
    "nombres_metadatos = [\"PROT\",\"TFAT\",\"CARB\",\"MOIS\",\"ALC\",\"CAFF\",\"THEO\",\"CALC\",\"MAGN\",\"POTA\",\"ZINC\",\"VC\",\"VB1\",\"VB6\",\"VARA\",\"ACAR\",\"CRYP\",\"LYCO\",\"ATOC\",\"VK\",\"CHOLE\",\"VITD\"]\n",
    "df_microbioma_train, df_microbioma_test, _, _, \\\n",
    "df_domain_train, df_domain_test, _, _, otu_columns, domain_columns = read_df(metadata_names=nombres_metadatos,otu_filename='resultados_ana/datos_otus_filtrados/otu_table_especies_80.csv',metadata_filename='resultados_ana/metadatos_nutrientes_filtrados_22.csv')\n",
    "\n",
    "data_microbioma_train = df_microbioma_train.to_numpy(dtype=np.float32)\n",
    "data_microbioma_test = df_microbioma_test.to_numpy(dtype=np.float32)\n",
    "data_domain_train = df_domain_train.to_numpy(dtype=np.float32)\n",
    "data_domain_test = df_domain_test.to_numpy(dtype=np.float32)\n",
    "\n",
    "#Preparamos las combinaciones pertinentes (5 mejores)\n",
    "#####LA he liado aqui, deberia ser otra, hay que volver a hacerlo\n",
    "combinations = [[100,64,0.001,optimizers.Adam,15,[512,256],\"tanh\",\"tanh\"],\\\n",
    "                [100,64,0.001,optimizers.Adam,15,[512,256],\"tanh\",\"relu\"],\\ #este deberia ser 96\n",
    "                [100,96,0.001,optimizers.Adam,15,[512,256,128],\"tanh\",\"tanh\"],\\ #deberia ser tanh, relu\n",
    "                [100,96,0.01,optimizers.Adam,15,[512,256],\"relu\",\"tanh\"],\\\n",
    "                [100,64,0.01,optimizers.Adam,10,[512,256],\"relu\",\"tanh\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-delay",
   "metadata": {},
   "source": [
    "### 0.001, tanh, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "referenced-robin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4666104316711426 | 0.4666104316711426 | 0.4666104316711426 |\n",
       "| pearson_corr | 0.6399815082550049 | 0.6399815082550049 | 0.6399815082550049 |\n",
       "| jensen_shannon_divergence | 1.2309167385101318 | 1.2309167385101318 | 1.2309167385101318 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4712810814380646 | 0.4712810814380646 | 0.4712810814380646 |\n",
       "| pearson_corr | 0.6133289933204651 | 0.6133289933204651 | 0.6133289933204651 |\n",
       "| jensen_shannon_divergence | 1.3000926971435547 | 1.3000926971435547 | 1.3000926971435547 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.46007561683654785 | 0.46007561683654785 | 0.46007561683654785 |\n",
       "| pearson_corr | 0.6344874501228333 | 0.6344874501228333 | 0.6344874501228333 |\n",
       "| jensen_shannon_divergence | 1.2321958541870117 | 1.2321958541870117 | 1.2321958541870117 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.49360331892967224 | 0.49360331892967224 | 0.49360331892967224 |\n",
       "| pearson_corr | 0.6167614459991455 | 0.6167614459991455 | 0.6167614459991455 |\n",
       "| jensen_shannon_divergence | 1.3982597589492798 | 1.3982597589492798 | 1.3982597589492798 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.46746793389320374 | 0.46746793389320374 | 0.46746793389320374 |\n",
       "| pearson_corr | 0.6413127183914185 | 0.6413127183914185 | 0.6413127183914185 |\n",
       "| jensen_shannon_divergence | 1.302670955657959 | 1.302670955657959 | 1.302670955657959 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation=None)(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-development",
   "metadata": {},
   "source": [
    "### 0.001, tanh, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accomplished-folks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.586988627910614 | 0.586988627910614 | 0.586988627910614 |\n",
       "| pearson_corr | 0.5942000150680542 | 0.5942000150680542 | 0.5942000150680542 |\n",
       "| jensen_shannon_divergence | 1.5406224727630615 | 1.5406224727630615 | 1.5406224727630615 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6788764595985413 | 0.6788764595985413 | 0.6788764595985413 |\n",
       "| pearson_corr | 0.40872642397880554 | 0.40872642397880554 | 0.40872642397880554 |\n",
       "| jensen_shannon_divergence | 2.1576194763183594 | 2.1576194763183594 | 2.1576194763183594 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.620358407497406 | 0.620358407497406 | 0.620358407497406 |\n",
       "| pearson_corr | 0.42078739404678345 | 0.42078739404678345 | 0.42078739404678345 |\n",
       "| jensen_shannon_divergence | 1.6858012676239014 | 1.6858012676239014 | 1.6858012676239014 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5924661755561829 | 0.5924661755561829 | 0.5924661755561829 |\n",
       "| pearson_corr | 0.4818921685218811 | 0.4818921685218811 | 0.4818921685218811 |\n",
       "| jensen_shannon_divergence | 1.7611680030822754 | 1.7611680030822754 | 1.7611680030822754 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.49580317735671997 | 0.49580317735671997 | 0.49580317735671997 |\n",
       "| pearson_corr | 0.6158543229103088 | 0.6158543229103088 | 0.6158543229103088 |\n",
       "| jensen_shannon_divergence | 1.405089020729065 | 1.405089020729065 | 1.405089020729065 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-airport",
   "metadata": {},
   "source": [
    "### 0.001, tanh, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "electoral-speaking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.532667338848114 | 0.532667338848114 | 0.532667338848114 |\n",
       "| pearson_corr | 0.5948635935783386 | 0.5948635935783386 | 0.5948635935783386 |\n",
       "| jensen_shannon_divergence | 1.3232924938201904 | 1.3232924938201904 | 1.3232924938201904 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4926283359527588 | 0.4926283359527588 | 0.4926283359527588 |\n",
       "| pearson_corr | 0.597537636756897 | 0.597537636756897 | 0.597537636756897 |\n",
       "| jensen_shannon_divergence | 1.2805906534194946 | 1.2805906534194946 | 1.2805906534194946 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6145093441009521 | 0.6145093441009521 | 0.6145093441009521 |\n",
       "| pearson_corr | 0.3691496253013611 | 0.3691496253013611 | 0.3691496253013611 |\n",
       "| jensen_shannon_divergence | 1.712808609008789 | 1.712808609008789 | 1.712808609008789 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5824371576309204 | 0.5824371576309204 | 0.5824371576309204 |\n",
       "| pearson_corr | 0.5245718955993652 | 0.5245718955993652 | 0.5245718955993652 |\n",
       "| jensen_shannon_divergence | 1.666982889175415 | 1.666982889175415 | 1.666982889175415 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47629714012145996 | 0.47629714012145996 | 0.47629714012145996 |\n",
       "| pearson_corr | 0.6428070664405823 | 0.6428070664405823 | 0.6428070664405823 |\n",
       "| jensen_shannon_divergence | 1.3273110389709473 | 1.3273110389709473 | 1.3273110389709473 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-tradition",
   "metadata": {},
   "source": [
    "### 0.001, tanh, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "progressive-soundtrack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5334022045135498 | 0.5334022045135498 | 0.5334022045135498 |\n",
       "| pearson_corr | 0.5845524668693542 | 0.5845524668693542 | 0.5845524668693542 |\n",
       "| jensen_shannon_divergence | 1.3018138408660889 | 1.3018138408660889 | 1.3018138408660889 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.480394572019577 | 0.480394572019577 | 0.480394572019577 |\n",
       "| pearson_corr | 0.6137431263923645 | 0.6137431263923645 | 0.6137431263923645 |\n",
       "| jensen_shannon_divergence | 1.311356544494629 | 1.311356544494629 | 1.311356544494629 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6104145646095276 | 0.6104145646095276 | 0.6104145646095276 |\n",
       "| pearson_corr | 0.39936313033103943 | 0.39936313033103943 | 0.39936313033103943 |\n",
       "| jensen_shannon_divergence | 1.678567886352539 | 1.678567886352539 | 1.678567886352539 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5763248801231384 | 0.5763248801231384 | 0.5763248801231384 |\n",
       "| pearson_corr | 0.5414748787879944 | 0.5414748787879944 | 0.5414748787879944 |\n",
       "| jensen_shannon_divergence | 1.6825300455093384 | 1.6825300455093384 | 1.6825300455093384 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4770861566066742 | 0.4770861566066742 | 0.4770861566066742 |\n",
       "| pearson_corr | 0.6317811012268066 | 0.6317811012268066 | 0.6317811012268066 |\n",
       "| jensen_shannon_divergence | 1.3847616910934448 | 1.3847616910934448 | 1.3847616910934448 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olympic-closer",
   "metadata": {},
   "source": [
    "### 0.001, softmax, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "prerequisite-accent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4749259054660797 | 0.4749259054660797 | 0.4749259054660797 |\n",
       "| pearson_corr | 0.6169785857200623 | 0.6169785857200623 | 0.6169785857200623 |\n",
       "| jensen_shannon_divergence | 1.2010539770126343 | 1.2010539770126343 | 1.2010539770126343 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4860888421535492 | 0.4860888421535492 | 0.4860888421535492 |\n",
       "| pearson_corr | 0.6321198344230652 | 0.6321198344230652 | 0.6321198344230652 |\n",
       "| jensen_shannon_divergence | 1.2382569313049316 | 1.2382569313049316 | 1.2382569313049316 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4825393855571747 | 0.4825393855571747 | 0.4825393855571747 |\n",
       "| pearson_corr | 0.6196183562278748 | 0.6196183562278748 | 0.6196183562278748 |\n",
       "| jensen_shannon_divergence | 1.2417892217636108 | 1.2417892217636108 | 1.2417892217636108 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5568448901176453 | 0.5568448901176453 | 0.5568448901176453 |\n",
       "| pearson_corr | 0.5595479607582092 | 0.5595479607582092 | 0.5595479607582092 |\n",
       "| jensen_shannon_divergence | 1.6037875413894653 | 1.6037875413894653 | 1.6037875413894653 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5011512041091919 | 0.5011512041091919 | 0.5011512041091919 |\n",
       "| pearson_corr | 0.594170868396759 | 0.594170868396759 | 0.594170868396759 |\n",
       "| jensen_shannon_divergence | 1.4357837438583374 | 1.4357837438583374 | 1.4357837438583374 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "  \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-meditation",
   "metadata": {},
   "source": [
    "### 0.001, softmax, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "competent-italic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5873225331306458 | 0.5873225331306458 | 0.5873225331306458 |\n",
       "| pearson_corr | 0.5695122480392456 | 0.5695122480392456 | 0.5695122480392456 |\n",
       "| jensen_shannon_divergence | 1.5207350254058838 | 1.5207350254058838 | 1.5207350254058838 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6751806139945984 | 0.6751806139945984 | 0.6751806139945984 |\n",
       "| pearson_corr | 0.40019071102142334 | 0.40019071102142334 | 0.40019071102142334 |\n",
       "| jensen_shannon_divergence | 2.1267290115356445 | 2.1267290115356445 | 2.1267290115356445 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6136248707771301 | 0.6136248707771301 | 0.6136248707771301 |\n",
       "| pearson_corr | 0.42584821581840515 | 0.42584821581840515 | 0.42584821581840515 |\n",
       "| jensen_shannon_divergence | 1.6336593627929688 | 1.6336593627929688 | 1.6336593627929688 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5904536843299866 | 0.5904536843299866 | 0.5904536843299866 |\n",
       "| pearson_corr | 0.4813489615917206 | 0.4813489615917206 | 0.4813489615917206 |\n",
       "| jensen_shannon_divergence | 1.7280718088150024 | 1.7280718088150024 | 1.7280718088150024 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4968138337135315 | 0.4968138337135315 | 0.4968138337135315 |\n",
       "| pearson_corr | 0.6168971061706543 | 0.6168971061706543 | 0.6168971061706543 |\n",
       "| jensen_shannon_divergence | 1.4220815896987915 | 1.4220815896987915 | 1.4220815896987915 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "\n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-building",
   "metadata": {},
   "source": [
    "### 0.001, softmax, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "changing-swift",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5424084663391113 | 0.5424084663391113 | 0.5424084663391113 |\n",
       "| pearson_corr | 0.5868552327156067 | 0.5868552327156067 | 0.5868552327156067 |\n",
       "| jensen_shannon_divergence | 1.3533568382263184 | 1.3533568382263184 | 1.3533568382263184 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4866507947444916 | 0.4866507947444916 | 0.4866507947444916 |\n",
       "| pearson_corr | 0.6318418383598328 | 0.6318418383598328 | 0.6318418383598328 |\n",
       "| jensen_shannon_divergence | 1.238396167755127 | 1.238396167755127 | 1.238396167755127 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.622290849685669 | 0.622290849685669 | 0.622290849685669 |\n",
       "| pearson_corr | 0.3455917239189148 | 0.3455917239189148 | 0.3455917239189148 |\n",
       "| jensen_shannon_divergence | 1.7254948616027832 | 1.7254948616027832 | 1.7254948616027832 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5908083319664001 | 0.5908083319664001 | 0.5908083319664001 |\n",
       "| pearson_corr | 0.4991909861564636 | 0.4991909861564636 | 0.4991909861564636 |\n",
       "| jensen_shannon_divergence | 1.6940926313400269 | 1.6940926313400269 | 1.6940926313400269 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4955788552761078 | 0.4955788552761078 | 0.4955788552761078 |\n",
       "| pearson_corr | 0.617987871170044 | 0.617987871170044 | 0.617987871170044 |\n",
       "| jensen_shannon_divergence | 1.420027256011963 | 1.420027256011963 | 1.420027256011963 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "\n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-rental",
   "metadata": {},
   "source": [
    "### 0.001, softmax, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "virtual-chicken",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5407905578613281 | 0.5407905578613281 | 0.5407905578613281 |\n",
       "| pearson_corr | 0.5861202478408813 | 0.5861202478408813 | 0.5861202478408813 |\n",
       "| jensen_shannon_divergence | 1.334692358970642 | 1.334692358970642 | 1.334692358970642 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5058145523071289 | 0.5058145523071289 | 0.5058145523071289 |\n",
       "| pearson_corr | 0.5995340347290039 | 0.5995340347290039 | 0.5995340347290039 |\n",
       "| jensen_shannon_divergence | 1.4027490615844727 | 1.4027490615844727 | 1.4027490615844727 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.612959623336792 | 0.612959623336792 | 0.612959623336792 |\n",
       "| pearson_corr | 0.4099174737930298 | 0.4099174737930298 | 0.4099174737930298 |\n",
       "| jensen_shannon_divergence | 1.7117822170257568 | 1.7117822170257568 | 1.7117822170257568 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5876970291137695 | 0.5876970291137695 | 0.5876970291137695 |\n",
       "| pearson_corr | 0.5043875575065613 | 0.5043875575065613 | 0.5043875575065613 |\n",
       "| jensen_shannon_divergence | 1.684457778930664 | 1.684457778930664 | 1.684457778930664 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5161761045455933 | 0.5161761045455933 | 0.5161761045455933 |\n",
       "| pearson_corr | 0.5866919755935669 | 0.5866919755935669 | 0.5866919755935669 |\n",
       "| jensen_shannon_divergence | 1.5428745746612549 | 1.5428745746612549 | 1.5428745746612549 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "\n",
    "\n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-egyptian",
   "metadata": {},
   "source": [
    "### 0.001, sigmoid, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bibliographic-silly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.45292574167251587 | 0.45292574167251587 | 0.45292574167251587 |\n",
       "| pearson_corr | 0.6506378650665283 | 0.6506378650665283 | 0.6506378650665283 |\n",
       "| jensen_shannon_divergence | 1.1149582862854004 | 1.1149582862854004 | 1.1149582862854004 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47075629234313965 | 0.47075629234313965 | 0.47075629234313965 |\n",
       "| pearson_corr | 0.6524497270584106 | 0.6524497270584106 | 0.6524497270584106 |\n",
       "| jensen_shannon_divergence | 1.1872199773788452 | 1.1872199773788452 | 1.1872199773788452 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4695640802383423 | 0.4695640802383423 | 0.4695640802383423 |\n",
       "| pearson_corr | 0.6375301480293274 | 0.6375301480293274 | 0.6375301480293274 |\n",
       "| jensen_shannon_divergence | 1.1975822448730469 | 1.1975822448730469 | 1.1975822448730469 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5234004855155945 | 0.5234004855155945 | 0.5234004855155945 |\n",
       "| pearson_corr | 0.6056731343269348 | 0.6056731343269348 | 0.6056731343269348 |\n",
       "| jensen_shannon_divergence | 1.4622228145599365 | 1.4622228145599365 | 1.4622228145599365 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4900318384170532 | 0.4900318384170532 | 0.4900318384170532 |\n",
       "| pearson_corr | 0.6163731217384338 | 0.6163731217384338 | 0.6163731217384338 |\n",
       "| jensen_shannon_divergence | 1.4021260738372803 | 1.4021260738372803 | 1.4021260738372803 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "\n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-designer",
   "metadata": {},
   "source": [
    "### 0.001, sigmoid, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "serial-mustang",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5852108597755432 | 0.5852108597755432 | 0.5852108597755432 |\n",
       "| pearson_corr | 0.5793415307998657 | 0.5793415307998657 | 0.5793415307998657 |\n",
       "| jensen_shannon_divergence | 1.5249993801116943 | 1.5249993801116943 | 1.5249993801116943 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6738884449005127 | 0.6738884449005127 | 0.6738884449005127 |\n",
       "| pearson_corr | 0.4024542570114136 | 0.4024542570114136 | 0.4024542570114136 |\n",
       "| jensen_shannon_divergence | 2.1107704639434814 | 2.1107704639434814 | 2.1107704639434814 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6136009097099304 | 0.6136009097099304 | 0.6136009097099304 |\n",
       "| pearson_corr | 0.42799490690231323 | 0.42799490690231323 | 0.42799490690231323 |\n",
       "| jensen_shannon_divergence | 1.6343995332717896 | 1.6343995332717896 | 1.6343995332717896 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5901432037353516 | 0.5901432037353516 | 0.5901432037353516 |\n",
       "| pearson_corr | 0.4834054708480835 | 0.4834054708480835 | 0.4834054708480835 |\n",
       "| jensen_shannon_divergence | 1.7236802577972412 | 1.7236802577972412 | 1.7236802577972412 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4955628216266632 | 0.4955628216266632 | 0.4955628216266632 |\n",
       "| pearson_corr | 0.6150037050247192 | 0.6150037050247192 | 0.6150037050247192 |\n",
       "| jensen_shannon_divergence | 1.4202455282211304 | 1.4202455282211304 | 1.4202455282211304 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "\n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-compound",
   "metadata": {},
   "source": [
    "### 0.001, sigmoid, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "liberal-packet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5408992767333984 | 0.5408992767333984 | 0.5408992767333984 |\n",
       "| pearson_corr | 0.5922964215278625 | 0.5922964215278625 | 0.5922964215278625 |\n",
       "| jensen_shannon_divergence | 1.346030592918396 | 1.346030592918396 | 1.346030592918396 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47584211826324463 | 0.47584211826324463 | 0.47584211826324463 |\n",
       "| pearson_corr | 0.6505153179168701 | 0.6505153179168701 | 0.6505153179168701 |\n",
       "| jensen_shannon_divergence | 1.1999902725219727 | 1.1999902725219727 | 1.1999902725219727 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6150209307670593 | 0.6150209307670593 | 0.6150209307670593 |\n",
       "| pearson_corr | 0.3654545843601227 | 0.3654545843601227 | 0.3654545843601227 |\n",
       "| jensen_shannon_divergence | 1.693184733390808 | 1.693184733390808 | 1.693184733390808 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5932461619377136 | 0.5932461619377136 | 0.5932461619377136 |\n",
       "| pearson_corr | 0.5024570822715759 | 0.5024570822715759 | 0.5024570822715759 |\n",
       "| jensen_shannon_divergence | 1.693668246269226 | 1.693668246269226 | 1.693668246269226 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.49753570556640625 | 0.49753570556640625 | 0.49753570556640625 |\n",
       "| pearson_corr | 0.6138851642608643 | 0.6138851642608643 | 0.6138851642608643 |\n",
       "| jensen_shannon_divergence | 1.426950454711914 | 1.426950454711914 | 1.426950454711914 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "\n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-musician",
   "metadata": {},
   "source": [
    "### 0.001, sigmoid, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "undefined-actress",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5329556465148926 | 0.5329556465148926 | 0.5329556465148926 |\n",
       "| pearson_corr | 0.6057089567184448 | 0.6057089567184448 | 0.6057089567184448 |\n",
       "| jensen_shannon_divergence | 1.2949601411819458 | 1.2949601411819458 | 1.2949601411819458 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4786314070224762 | 0.4786314070224762 | 0.4786314070224762 |\n",
       "| pearson_corr | 0.6247378587722778 | 0.6247378587722778 | 0.6247378587722778 |\n",
       "| jensen_shannon_divergence | 1.3116462230682373 | 1.3116462230682373 | 1.3116462230682373 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6074930429458618 | 0.6074930429458618 | 0.6074930429458618 |\n",
       "| pearson_corr | 0.42119842767715454 | 0.42119842767715454 | 0.42119842767715454 |\n",
       "| jensen_shannon_divergence | 1.7008439302444458 | 1.7008439302444458 | 1.7008439302444458 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.588212788105011 | 0.588212788105011 | 0.588212788105011 |\n",
       "| pearson_corr | 0.5013874769210815 | 0.5013874769210815 | 0.5013874769210815 |\n",
       "| jensen_shannon_divergence | 1.7104216814041138 | 1.7104216814041138 | 1.7104216814041138 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5091392397880554 | 0.5091392397880554 | 0.5091392397880554 |\n",
       "| pearson_corr | 0.5934340953826904 | 0.5934340953826904 | 0.5934340953826904 |\n",
       "| jensen_shannon_divergence | 1.5164493322372437 | 1.5164493322372437 | 1.5164493322372437 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-hacker",
   "metadata": {},
   "source": [
    "### 0.001, relu, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "mobile-fitness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6219472885131836 | 0.6219472885131836 | 0.6219472885131836 |\n",
       "| pearson_corr | 0.5768758654594421 | 0.5768758654594421 | 0.5768758654594421 |\n",
       "| jensen_shannon_divergence | 2.4853711128234863 | 2.4853711128234863 | 2.4853711128234863 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6728084087371826 | 0.6728084087371826 | 0.6728084087371826 |\n",
       "| pearson_corr | 0.40082213282585144 | 0.40082213282585144 | 0.40082213282585144 |\n",
       "| jensen_shannon_divergence | 2.2855660915374756 | 2.2855660915374756 | 2.2855660915374756 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8020629286766052 | 0.8020629286766052 | 0.8020629286766052 |\n",
       "| pearson_corr | 0.12882716953754425 | 0.12882716953754425 | 0.12882716953754425 |\n",
       "| jensen_shannon_divergence | 3.717818260192871 | 3.717818260192871 | 3.717818260192871 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7280701398849487 | 0.7280701398849487 | 0.7280701398849487 |\n",
       "| pearson_corr | 0.37260672450065613 | 0.37260672450065613 | 0.37260672450065613 |\n",
       "| jensen_shannon_divergence | 3.200467586517334 | 3.200467586517334 | 3.200467586517334 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6892411112785339 | 0.6892411112785339 | 0.6892411112785339 |\n",
       "| pearson_corr | 0.29762476682662964 | 0.29762476682662964 | 0.29762476682662964 |\n",
       "| jensen_shannon_divergence | 3.4883010387420654 | 3.4883010387420654 | 3.4883010387420654 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-billy",
   "metadata": {},
   "source": [
    "### 0.001, relu, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "raising-president",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.722378671169281 | 0.722378671169281 | 0.722378671169281 |\n",
       "| pearson_corr | 0.3621353805065155 | 0.3621353805065155 | 0.3621353805065155 |\n",
       "| jensen_shannon_divergence | 2.727346181869507 | 2.727346181869507 | 2.727346181869507 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8037365674972534 | 0.8037365674972534 | 0.8037365674972534 |\n",
       "| pearson_corr | 0.12278395146131516 | 0.12278395146131516 | 0.12278395146131516 |\n",
       "| jensen_shannon_divergence | 3.5906176567077637 | 3.5906176567077637 | 3.5906176567077637 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6070936322212219 | 0.6070936322212219 | 0.6070936322212219 |\n",
       "| pearson_corr | 0.4273258447647095 | 0.4273258447647095 | 0.4273258447647095 |\n",
       "| jensen_shannon_divergence | 1.622607946395874 | 1.622607946395874 | 1.622607946395874 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7197839617729187 | 0.7197839617729187 | 0.7197839617729187 |\n",
       "| pearson_corr | 0.18534798920154572 | 0.18534798920154572 | 0.18534798920154572 |\n",
       "| jensen_shannon_divergence | 2.555655002593994 | 2.555655002593994 | 2.555655002593994 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5335311889648438 | 0.5335311889648438 | 0.5335311889648438 |\n",
       "| pearson_corr | 0.5180814862251282 | 0.5180814862251282 | 0.5180814862251282 |\n",
       "| jensen_shannon_divergence | 1.6139085292816162 | 1.6139085292816162 | 1.6139085292816162 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-census",
   "metadata": {},
   "source": [
    "### 0.001, relu, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "enabling-district",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6116445660591125 | 0.6116445660591125 | 0.6116445660591125 |\n",
       "| pearson_corr | 0.423698753118515 | 0.423698753118515 | 0.423698753118515 |\n",
       "| jensen_shannon_divergence | 1.85905921459198 | 1.85905921459198 | 1.85905921459198 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5753889083862305 | 0.5753889083862305 | 0.5753889083862305 |\n",
       "| pearson_corr | 0.4730903208255768 | 0.4730903208255768 | 0.4730903208255768 |\n",
       "| jensen_shannon_divergence | 1.5888479948043823 | 1.5888479948043823 | 1.5888479948043823 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7391718626022339 | 0.7391718626022339 | 0.7391718626022339 |\n",
       "| pearson_corr | 0.16020190715789795 | 0.16020190715789795 | 0.16020190715789795 |\n",
       "| jensen_shannon_divergence | 2.756600856781006 | 2.756600856781006 | 2.756600856781006 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6016227006912231 | 0.6016227006912231 | 0.6016227006912231 |\n",
       "| pearson_corr | 0.3993355333805084 | 0.3993355333805084 | 0.3993355333805084 |\n",
       "| jensen_shannon_divergence | 1.6724246740341187 | 1.6724246740341187 | 1.6724246740341187 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5282157063484192 | 0.5282157063484192 | 0.5282157063484192 |\n",
       "| pearson_corr | 0.5559995174407959 | 0.5559995174407959 | 0.5559995174407959 |\n",
       "| jensen_shannon_divergence | 1.5815693140029907 | 1.5815693140029907 | 1.5815693140029907 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-right",
   "metadata": {},
   "source": [
    "### 0.001, relu, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "confident-finland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7541691660881042 | 0.7541691660881042 | 0.7541691660881042 |\n",
       "| pearson_corr | 0.39303353428840637 | 0.39303353428840637 | 0.39303353428840637 |\n",
       "| jensen_shannon_divergence | 2.7582242488861084 | 2.7582242488861084 | 2.7582242488861084 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7887640595436096 | 0.7887640595436096 | 0.7887640595436096 |\n",
       "| pearson_corr | 0.20393438637256622 | 0.20393438637256622 | 0.20393438637256622 |\n",
       "| jensen_shannon_divergence | 3.34212327003479 | 3.34212327003479 | 3.34212327003479 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7510498762130737 | 0.7510498762130737 | 0.7510498762130737 |\n",
       "| pearson_corr | 0.36576181650161743 | 0.36576181650161743 | 0.36576181650161743 |\n",
       "| jensen_shannon_divergence | 2.734800338745117 | 2.734800338745117 | 2.734800338745117 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6499668955802917 | 0.6499668955802917 | 0.6499668955802917 |\n",
       "| pearson_corr | 0.34821632504463196 | 0.34821632504463196 | 0.34821632504463196 |\n",
       "| jensen_shannon_divergence | 2.0826337337493896 | 2.0826337337493896 | 2.0826337337493896 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5357425212860107 | 0.5357425212860107 | 0.5357425212860107 |\n",
       "| pearson_corr | 0.5860261917114258 | 0.5860261917114258 | 0.5860261917114258 |\n",
       "| jensen_shannon_divergence | 1.6744686365127563 | 1.6744686365127563 | 1.6744686365127563 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-brazilian",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "### 0.01, tanh, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "proved-nitrogen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.474091112613678 | 0.474091112613678 | 0.474091112613678 |\n",
       "| pearson_corr | 0.6171327233314514 | 0.6171327233314514 | 0.6171327233314514 |\n",
       "| jensen_shannon_divergence | 1.219081997871399 | 1.219081997871399 | 1.219081997871399 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4869934320449829 | 0.4869934320449829 | 0.4869934320449829 |\n",
       "| pearson_corr | 0.6280184388160706 | 0.6280184388160706 | 0.6280184388160706 |\n",
       "| jensen_shannon_divergence | 1.2627395391464233 | 1.2627395391464233 | 1.2627395391464233 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4894231855869293 | 0.4894231855869293 | 0.4894231855869293 |\n",
       "| pearson_corr | 0.6167229413986206 | 0.6167229413986206 | 0.6167229413986206 |\n",
       "| jensen_shannon_divergence | 1.2990741729736328 | 1.2990741729736328 | 1.2990741729736328 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.568156898021698 | 0.568156898021698 | 0.568156898021698 |\n",
       "| pearson_corr | 0.5312444567680359 | 0.5312444567680359 | 0.5312444567680359 |\n",
       "| jensen_shannon_divergence | 1.6486533880233765 | 1.6486533880233765 | 1.6486533880233765 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.49966999888420105 | 0.49966999888420105 | 0.49966999888420105 |\n",
       "| pearson_corr | 0.5936757922172546 | 0.5936757922172546 | 0.5936757922172546 |\n",
       "| jensen_shannon_divergence | 1.4305322170257568 | 1.4305322170257568 | 1.4305322170257568 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-monroe",
   "metadata": {},
   "source": [
    "### 0.01, tanh, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "amended-driver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5890068411827087 | 0.5890068411827087 | 0.5890068411827087 |\n",
       "| pearson_corr | 0.5695511102676392 | 0.5695511102676392 | 0.5695511102676392 |\n",
       "| jensen_shannon_divergence | 1.5309932231903076 | 1.5309932231903076 | 1.5309932231903076 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6717161536216736 | 0.6717161536216736 | 0.6717161536216736 |\n",
       "| pearson_corr | 0.40583059191703796 | 0.40583059191703796 | 0.40583059191703796 |\n",
       "| jensen_shannon_divergence | 2.106616497039795 | 2.106616497039795 | 2.106616497039795 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6141913533210754 | 0.6141913533210754 | 0.6141913533210754 |\n",
       "| pearson_corr | 0.4239046275615692 | 0.4239046275615692 | 0.4239046275615692 |\n",
       "| jensen_shannon_divergence | 1.6399766206741333 | 1.6399766206741333 | 1.6399766206741333 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5912229418754578 | 0.5912229418754578 | 0.5912229418754578 |\n",
       "| pearson_corr | 0.481029212474823 | 0.481029212474823 | 0.481029212474823 |\n",
       "| jensen_shannon_divergence | 1.7280992269515991 | 1.7280992269515991 | 1.7280992269515991 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.49611330032348633 | 0.49611330032348633 | 0.49611330032348633 |\n",
       "| pearson_corr | 0.6183952689170837 | 0.6183952689170837 | 0.6183952689170837 |\n",
       "| jensen_shannon_divergence | 1.418558120727539 | 1.418558120727539 | 1.418558120727539 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-conservative",
   "metadata": {},
   "source": [
    "### 0.01, tanh, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "incorrect-deadline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5424045324325562 | 0.5424045324325562 | 0.5424045324325562 |\n",
       "| pearson_corr | 0.5869810581207275 | 0.5869810581207275 | 0.5869810581207275 |\n",
       "| jensen_shannon_divergence | 1.3508694171905518 | 1.3508694171905518 | 1.3508694171905518 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47769632935523987 | 0.47769632935523987 | 0.47769632935523987 |\n",
       "| pearson_corr | 0.6396298408508301 | 0.6396298408508301 | 0.6396298408508301 |\n",
       "| jensen_shannon_divergence | 1.2200433015823364 | 1.2200433015823364 | 1.2200433015823364 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.614893913269043 | 0.614893913269043 | 0.614893913269043 |\n",
       "| pearson_corr | 0.3717440366744995 | 0.3717440366744995 | 0.3717440366744995 |\n",
       "| jensen_shannon_divergence | 1.7028127908706665 | 1.7028127908706665 | 1.7028127908706665 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5933905839920044 | 0.5933905839920044 | 0.5933905839920044 |\n",
       "| pearson_corr | 0.4882001578807831 | 0.4882001578807831 | 0.4882001578807831 |\n",
       "| jensen_shannon_divergence | 1.7133604288101196 | 1.7133604288101196 | 1.7133604288101196 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4963834583759308 | 0.4963834583759308 | 0.4963834583759308 |\n",
       "| pearson_corr | 0.6168468594551086 | 0.6168468594551086 | 0.6168468594551086 |\n",
       "| jensen_shannon_divergence | 1.4211859703063965 | 1.4211859703063965 | 1.4211859703063965 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-patrol",
   "metadata": {},
   "source": [
    "### 0.01, tanh, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "electoral-walker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5442017912864685 | 0.5442017912864685 | 0.5442017912864685 |\n",
       "| pearson_corr | 0.5801864266395569 | 0.5801864266395569 | 0.5801864266395569 |\n",
       "| jensen_shannon_divergence | 1.3442027568817139 | 1.3442027568817139 | 1.3442027568817139 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47905945777893066 | 0.47905945777893066 | 0.47905945777893066 |\n",
       "| pearson_corr | 0.6278253793716431 | 0.6278253793716431 | 0.6278253793716431 |\n",
       "| jensen_shannon_divergence | 1.28011155128479 | 1.28011155128479 | 1.28011155128479 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6356729865074158 | 0.6356729865074158 | 0.6356729865074158 |\n",
       "| pearson_corr | 0.34486398100852966 | 0.34486398100852966 | 0.34486398100852966 |\n",
       "| jensen_shannon_divergence | 1.791782259941101 | 1.791782259941101 | 1.791782259941101 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6472703218460083 | 0.6472703218460083 | 0.6472703218460083 |\n",
       "| pearson_corr | 0.3543688952922821 | 0.3543688952922821 | 0.3543688952922821 |\n",
       "| jensen_shannon_divergence | 2.0513923168182373 | 2.0513923168182373 | 2.0513923168182373 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4927811026573181 | 0.4927811026573181 | 0.4927811026573181 |\n",
       "| pearson_corr | 0.6283842921257019 | 0.6283842921257019 | 0.6283842921257019 |\n",
       "| jensen_shannon_divergence | 1.3787132501602173 | 1.3787132501602173 | 1.3787132501602173 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-software",
   "metadata": {},
   "source": [
    "### 0.01, softmax, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "certain-nebraska",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47329938411712646 | 0.47329938411712646 | 0.47329938411712646 |\n",
       "| pearson_corr | 0.6189389824867249 | 0.6189389824867249 | 0.6189389824867249 |\n",
       "| jensen_shannon_divergence | 1.1994370222091675 | 1.1994370222091675 | 1.1994370222091675 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.48165759444236755 | 0.48165759444236755 | 0.48165759444236755 |\n",
       "| pearson_corr | 0.6362542510032654 | 0.6362542510032654 | 0.6362542510032654 |\n",
       "| jensen_shannon_divergence | 1.2282445430755615 | 1.2282445430755615 | 1.2282445430755615 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4833216667175293 | 0.4833216667175293 | 0.4833216667175293 |\n",
       "| pearson_corr | 0.6176357865333557 | 0.6176357865333557 | 0.6176357865333557 |\n",
       "| jensen_shannon_divergence | 1.2482051849365234 | 1.2482051849365234 | 1.2482051849365234 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5574375987052917 | 0.5574375987052917 | 0.5574375987052917 |\n",
       "| pearson_corr | 0.5553440451622009 | 0.5553440451622009 | 0.5553440451622009 |\n",
       "| jensen_shannon_divergence | 1.606074571609497 | 1.606074571609497 | 1.606074571609497 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5017693638801575 | 0.5017693638801575 | 0.5017693638801575 |\n",
       "| pearson_corr | 0.5955280661582947 | 0.5955280661582947 | 0.5955280661582947 |\n",
       "| jensen_shannon_divergence | 1.4412851333618164 | 1.4412851333618164 | 1.4412851333618164 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-nothing",
   "metadata": {},
   "source": [
    "### 0.01, softmax, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "final-greene",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5884733200073242 | 0.5884733200073242 | 0.5884733200073242 |\n",
       "| pearson_corr | 0.5692443251609802 | 0.5692443251609802 | 0.5692443251609802 |\n",
       "| jensen_shannon_divergence | 1.527906894683838 | 1.527906894683838 | 1.527906894683838 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6723020672798157 | 0.6723020672798157 | 0.6723020672798157 |\n",
       "| pearson_corr | 0.4022471308708191 | 0.4022471308708191 | 0.4022471308708191 |\n",
       "| jensen_shannon_divergence | 2.1093900203704834 | 2.1093900203704834 | 2.1093900203704834 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6132334470748901 | 0.6132334470748901 | 0.6132334470748901 |\n",
       "| pearson_corr | 0.42760229110717773 | 0.42760229110717773 | 0.42760229110717773 |\n",
       "| jensen_shannon_divergence | 1.6329764127731323 | 1.6329764127731323 | 1.6329764127731323 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5913715958595276 | 0.5913715958595276 | 0.5913715958595276 |\n",
       "| pearson_corr | 0.4817619025707245 | 0.4817619025707245 | 0.4817619025707245 |\n",
       "| jensen_shannon_divergence | 1.7258583307266235 | 1.7258583307266235 | 1.7258583307266235 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4971248507499695 | 0.4971248507499695 | 0.4971248507499695 |\n",
       "| pearson_corr | 0.6158589124679565 | 0.6158589124679565 | 0.6158589124679565 |\n",
       "| jensen_shannon_divergence | 1.4244366884231567 | 1.4244366884231567 | 1.4244366884231567 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-pension",
   "metadata": {},
   "source": [
    "### 0.01, softmax, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "prescription-wallace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5413151979446411 | 0.5413151979446411 | 0.5413151979446411 |\n",
       "| pearson_corr | 0.5919100642204285 | 0.5919100642204285 | 0.5919100642204285 |\n",
       "| jensen_shannon_divergence | 1.3468931913375854 | 1.3468931913375854 | 1.3468931913375854 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4777947664260864 | 0.4777947664260864 | 0.4777947664260864 |\n",
       "| pearson_corr | 0.6444529294967651 | 0.6444529294967651 | 0.6444529294967651 |\n",
       "| jensen_shannon_divergence | 1.2101408243179321 | 1.2101408243179321 | 1.2101408243179321 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6149714589118958 | 0.6149714589118958 | 0.6149714589118958 |\n",
       "| pearson_corr | 0.36591672897338867 | 0.36591672897338867 | 0.36591672897338867 |\n",
       "| jensen_shannon_divergence | 1.693376064300537 | 1.693376064300537 | 1.693376064300537 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5935360789299011 | 0.5935360789299011 | 0.5935360789299011 |\n",
       "| pearson_corr | 0.5019572973251343 | 0.5019572973251343 | 0.5019572973251343 |\n",
       "| jensen_shannon_divergence | 1.6946349143981934 | 1.6946349143981934 | 1.6946349143981934 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4976891577243805 | 0.4976891577243805 | 0.4976891577243805 |\n",
       "| pearson_corr | 0.6135696768760681 | 0.6135696768760681 | 0.6135696768760681 |\n",
       "| jensen_shannon_divergence | 1.4278128147125244 | 1.4278128147125244 | 1.4278128147125244 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-montgomery",
   "metadata": {},
   "source": [
    "### 0.01, softmax, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "guilty-disorder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.544954240322113 | 0.544954240322113 | 0.544954240322113 |\n",
       "| pearson_corr | 0.5806062817573547 | 0.5806062817573547 | 0.5806062817573547 |\n",
       "| jensen_shannon_divergence | 1.3496800661087036 | 1.3496800661087036 | 1.3496800661087036 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.488530695438385 | 0.488530695438385 | 0.488530695438385 |\n",
       "| pearson_corr | 0.6209768056869507 | 0.6209768056869507 | 0.6209768056869507 |\n",
       "| jensen_shannon_divergence | 1.3507972955703735 | 1.3507972955703735 | 1.3507972955703735 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6159145832061768 | 0.6159145832061768 | 0.6159145832061768 |\n",
       "| pearson_corr | 0.4061680734157562 | 0.4061680734157562 | 0.4061680734157562 |\n",
       "| jensen_shannon_divergence | 1.730709433555603 | 1.730709433555603 | 1.730709433555603 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5887959003448486 | 0.5887959003448486 | 0.5887959003448486 |\n",
       "| pearson_corr | 0.49505120515823364 | 0.49505120515823364 | 0.49505120515823364 |\n",
       "| jensen_shannon_divergence | 1.701473593711853 | 1.701473593711853 | 1.701473593711853 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5225422382354736 | 0.5225422382354736 | 0.5225422382354736 |\n",
       "| pearson_corr | 0.5760942697525024 | 0.5760942697525024 | 0.5760942697525024 |\n",
       "| jensen_shannon_divergence | 1.5785287618637085 | 1.5785287618637085 | 1.5785287618637085 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-stake",
   "metadata": {},
   "source": [
    "### 0.01, sigmoid, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "expressed-collins",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.46412286162376404 | 0.46412286162376404 | 0.46412286162376404 |\n",
       "| pearson_corr | 0.6297613978385925 | 0.6297613978385925 | 0.6297613978385925 |\n",
       "| jensen_shannon_divergence | 1.167609691619873 | 1.167609691619873 | 1.167609691619873 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.46452510356903076 | 0.46452510356903076 | 0.46452510356903076 |\n",
       "| pearson_corr | 0.6662938594818115 | 0.6662938594818115 | 0.6662938594818115 |\n",
       "| jensen_shannon_divergence | 1.16500985622406 | 1.16500985622406 | 1.16500985622406 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4834190607070923 | 0.4834190607070923 | 0.4834190607070923 |\n",
       "| pearson_corr | 0.6181965470314026 | 0.6181965470314026 | 0.6181965470314026 |\n",
       "| jensen_shannon_divergence | 1.248649001121521 | 1.248649001121521 | 1.248649001121521 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5488762259483337 | 0.5488762259483337 | 0.5488762259483337 |\n",
       "| pearson_corr | 0.5711533427238464 | 0.5711533427238464 | 0.5711533427238464 |\n",
       "| jensen_shannon_divergence | 1.563284158706665 | 1.563284158706665 | 1.563284158706665 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4942728281021118 | 0.4942728281021118 | 0.4942728281021118 |\n",
       "| pearson_corr | 0.6068559885025024 | 0.6068559885025024 | 0.6068559885025024 |\n",
       "| jensen_shannon_divergence | 1.4137170314788818 | 1.4137170314788818 | 1.4137170314788818 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-hierarchy",
   "metadata": {},
   "source": [
    "### 0.01, sigmoid, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "accessible-danish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5890363454818726 | 0.5890363454818726 | 0.5890363454818726 |\n",
       "| pearson_corr | 0.5696003437042236 | 0.5696003437042236 | 0.5696003437042236 |\n",
       "| jensen_shannon_divergence | 1.531173586845398 | 1.531173586845398 | 1.531173586845398 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6721305251121521 | 0.6721305251121521 | 0.6721305251121521 |\n",
       "| pearson_corr | 0.40291932225227356 | 0.40291932225227356 | 0.40291932225227356 |\n",
       "| jensen_shannon_divergence | 2.108981132507324 | 2.108981132507324 | 2.108981132507324 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6131136417388916 | 0.6131136417388916 | 0.6131136417388916 |\n",
       "| pearson_corr | 0.4257648289203644 | 0.4257648289203644 | 0.4257648289203644 |\n",
       "| jensen_shannon_divergence | 1.6338320970535278 | 1.6338320970535278 | 1.6338320970535278 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5918797850608826 | 0.5918797850608826 | 0.5918797850608826 |\n",
       "| pearson_corr | 0.4821167290210724 | 0.4821167290210724 | 0.4821167290210724 |\n",
       "| jensen_shannon_divergence | 1.7253587245941162 | 1.7253587245941162 | 1.7253587245941162 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4950781762599945 | 0.4950781762599945 | 0.4950781762599945 |\n",
       "| pearson_corr | 0.6190834641456604 | 0.6190834641456604 | 0.6190834641456604 |\n",
       "| jensen_shannon_divergence | 1.413927435874939 | 1.413927435874939 | 1.413927435874939 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-bread",
   "metadata": {},
   "source": [
    "### 0.01, sigmoid, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "occasional-pendant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5428716540336609 | 0.5428716540336609 | 0.5428716540336609 |\n",
       "| pearson_corr | 0.5864921808242798 | 0.5864921808242798 | 0.5864921808242798 |\n",
       "| jensen_shannon_divergence | 1.3526524305343628 | 1.3526524305343628 | 1.3526524305343628 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4806676506996155 | 0.4806676506996155 | 0.4806676506996155 |\n",
       "| pearson_corr | 0.637134850025177 | 0.637134850025177 | 0.637134850025177 |\n",
       "| jensen_shannon_divergence | 1.2266535758972168 | 1.2266535758972168 | 1.2266535758972168 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6147621273994446 | 0.6147621273994446 | 0.6147621273994446 |\n",
       "| pearson_corr | 0.37145256996154785 | 0.37145256996154785 | 0.37145256996154785 |\n",
       "| jensen_shannon_divergence | 1.7002612352371216 | 1.7002612352371216 | 1.7002612352371216 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5933076739311218 | 0.5933076739311218 | 0.5933076739311218 |\n",
       "| pearson_corr | 0.4907061457633972 | 0.4907061457633972 | 0.4907061457633972 |\n",
       "| jensen_shannon_divergence | 1.7087472677230835 | 1.7087472677230835 | 1.7087472677230835 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4973767399787903 | 0.4973767399787903 | 0.4973767399787903 |\n",
       "| pearson_corr | 0.6145907640457153 | 0.6145907640457153 | 0.6145907640457153 |\n",
       "| jensen_shannon_divergence | 1.4251785278320312 | 1.4251785278320312 | 1.4251785278320312 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-reading",
   "metadata": {},
   "source": [
    "### 0.01, sigmoid, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "outstanding-bradford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5464217662811279 | 0.5464217662811279 | 0.5464217662811279 |\n",
       "| pearson_corr | 0.5759163498878479 | 0.5759163498878479 | 0.5759163498878479 |\n",
       "| jensen_shannon_divergence | 1.3523391485214233 | 1.3523391485214233 | 1.3523391485214233 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.48840978741645813 | 0.48840978741645813 | 0.48840978741645813 |\n",
       "| pearson_corr | 0.6339693069458008 | 0.6339693069458008 | 0.6339693069458008 |\n",
       "| jensen_shannon_divergence | 1.2972055673599243 | 1.2972055673599243 | 1.2972055673599243 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6184991598129272 | 0.6184991598129272 | 0.6184991598129272 |\n",
       "| pearson_corr | 0.39902085065841675 | 0.39902085065841675 | 0.39902085065841675 |\n",
       "| jensen_shannon_divergence | 1.7475106716156006 | 1.7475106716156006 | 1.7475106716156006 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5863692760467529 | 0.5863692760467529 | 0.5863692760467529 |\n",
       "| pearson_corr | 0.4665094017982483 | 0.4665094017982483 | 0.4665094017982483 |\n",
       "| jensen_shannon_divergence | 1.8176243305206299 | 1.8176243305206299 | 1.8176243305206299 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4998079538345337 | 0.4998079538345337 | 0.4998079538345337 |\n",
       "| pearson_corr | 0.6131545901298523 | 0.6131545901298523 | 0.6131545901298523 |\n",
       "| jensen_shannon_divergence | 1.4269328117370605 | 1.4269328117370605 | 1.4269328117370605 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-plaza",
   "metadata": {},
   "source": [
    "### 0.01, relu, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "textile-cable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5925171375274658 | 0.5925171375274658 | 0.5925171375274658 |\n",
       "| pearson_corr | 0.5802437663078308 | 0.5802437663078308 | 0.5802437663078308 |\n",
       "| jensen_shannon_divergence | 2.2235240936279297 | 2.2235240936279297 | 2.2235240936279297 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7096750736236572 | 0.7096750736236572 | 0.7096750736236572 |\n",
       "| pearson_corr | 0.36807700991630554 | 0.36807700991630554 | 0.36807700991630554 |\n",
       "| jensen_shannon_divergence | 2.7388877868652344 | 2.7388877868652344 | 2.7388877868652344 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.9440853595733643 | 0.9440853595733643 | 0.9440853595733643 |\n",
       "| pearson_corr | 0.0077353185042738914 | 0.0077353185042738914 | 0.0077353185042738914 |\n",
       "| jensen_shannon_divergence | 6.754090785980225 | 6.754090785980225 | 6.754090785980225 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.900239109992981 | 0.900239109992981 | 0.900239109992981 |\n",
       "| pearson_corr | 0.11126795411109924 | 0.11126795411109924 | 0.11126795411109924 |\n",
       "| jensen_shannon_divergence | 6.5262131690979 | 6.5262131690979 | 6.5262131690979 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7095433473587036 | 0.7095433473587036 | 0.7095433473587036 |\n",
       "| pearson_corr | 0.21035796403884888 | 0.21035796403884888 | 0.21035796403884888 |\n",
       "| jensen_shannon_divergence | 3.391899347305298 | 3.391899347305298 | 3.391899347305298 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-groove",
   "metadata": {},
   "source": [
    "### 0.01, relu, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "taken-brother",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.722378671169281 | 0.722378671169281 | 0.722378671169281 |\n",
       "| pearson_corr | 0.3621353805065155 | 0.3621353805065155 | 0.3621353805065155 |\n",
       "| jensen_shannon_divergence | 2.727346181869507 | 2.727346181869507 | 2.727346181869507 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6920798420906067 | 0.6920798420906067 | 0.6920798420906067 |\n",
       "| pearson_corr | 0.32933932542800903 | 0.32933932542800903 | 0.32933932542800903 |\n",
       "| jensen_shannon_divergence | 2.3396284580230713 | 2.3396284580230713 | 2.3396284580230713 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6070936322212219 | 0.6070936322212219 | 0.6070936322212219 |\n",
       "| pearson_corr | 0.4273258447647095 | 0.4273258447647095 | 0.4273258447647095 |\n",
       "| jensen_shannon_divergence | 1.622607946395874 | 1.622607946395874 | 1.622607946395874 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5881101489067078 | 0.5881101489067078 | 0.5881101489067078 |\n",
       "| pearson_corr | 0.45865598320961 | 0.45865598320961 | 0.45865598320961 |\n",
       "| jensen_shannon_divergence | 1.8767889738082886 | 1.8767889738082886 | 1.8767889738082886 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5335311889648438 | 0.5335311889648438 | 0.5335311889648438 |\n",
       "| pearson_corr | 0.5180814862251282 | 0.5180814862251282 | 0.5180814862251282 |\n",
       "| jensen_shannon_divergence | 1.6139085292816162 | 1.6139085292816162 | 1.6139085292816162 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-qatar",
   "metadata": {},
   "source": [
    "### 0.01, relu, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "heard-wichita",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6675030589103699 | 0.6675030589103699 | 0.6675030589103699 |\n",
       "| pearson_corr | 0.3960495889186859 | 0.3960495889186859 | 0.3960495889186859 |\n",
       "| jensen_shannon_divergence | 2.0206642150878906 | 2.0206642150878906 | 2.0206642150878906 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5753889083862305 | 0.5753889083862305 | 0.5753889083862305 |\n",
       "| pearson_corr | 0.4730903208255768 | 0.4730903208255768 | 0.4730903208255768 |\n",
       "| jensen_shannon_divergence | 1.5888479948043823 | 1.5888479948043823 | 1.5888479948043823 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8085106611251831 | 0.8085106611251831 | 0.8085106611251831 |\n",
       "| pearson_corr | 0.062083326280117035 | 0.062083326280117035 | 0.062083326280117035 |\n",
       "| jensen_shannon_divergence | 3.5282981395721436 | 3.5282981395721436 | 3.5282981395721436 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6135459542274475 | 0.6135459542274475 | 0.6135459542274475 |\n",
       "| pearson_corr | 0.34997785091400146 | 0.34997785091400146 | 0.34997785091400146 |\n",
       "| jensen_shannon_divergence | 1.9693576097488403 | 1.9693576097488403 | 1.9693576097488403 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6403829455375671 | 0.6403829455375671 | 0.6403829455375671 |\n",
       "| pearson_corr | 0.3469322621822357 | 0.3469322621822357 | 0.3469322621822357 |\n",
       "| jensen_shannon_divergence | 1.9785798788070679 | 1.9785798788070679 | 1.9785798788070679 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-sullivan",
   "metadata": {},
   "source": [
    "### 0.01, relu, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "hungarian-artist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7534201741218567 | 0.7534201741218567 | 0.7534201741218567 |\n",
       "| pearson_corr | 0.39772671461105347 | 0.39772671461105347 | 0.39772671461105347 |\n",
       "| jensen_shannon_divergence | 2.7222695350646973 | 2.7222695350646973 | 2.7222695350646973 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7881259322166443 | 0.7881259322166443 | 0.7881259322166443 |\n",
       "| pearson_corr | 0.22082915902137756 | 0.22082915902137756 | 0.22082915902137756 |\n",
       "| jensen_shannon_divergence | 3.173801898956299 | 3.173801898956299 | 3.173801898956299 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7455583810806274 | 0.7455583810806274 | 0.7455583810806274 |\n",
       "| pearson_corr | 0.3798735737800598 | 0.3798735737800598 | 0.3798735737800598 |\n",
       "| jensen_shannon_divergence | 2.636539936065674 | 2.636539936065674 | 2.636539936065674 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6472703218460083 | 0.6472703218460083 | 0.6472703218460083 |\n",
       "| pearson_corr | 0.3543688952922821 | 0.3543688952922821 | 0.3543688952922821 |\n",
       "| jensen_shannon_divergence | 2.0513923168182373 | 2.0513923168182373 | 2.0513923168182373 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 10, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5292101502418518 | 0.5292101502418518 | 0.5292101502418518 |\n",
       "| pearson_corr | 0.5924198627471924 | 0.5924198627471924 | 0.5924198627471924 |\n",
       "| jensen_shannon_divergence | 1.5824764966964722 | 1.5824764966964722 | 1.5824764966964722 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
