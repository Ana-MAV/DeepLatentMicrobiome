{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "banner-september",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid de la segunda parte, en la que utilizamos ya la primera parte bien\n",
    "#Esta va a ser la de las 44 variables con combined\n",
    "\n",
    "\n",
    "#importamos paquetes\n",
    "import sys\n",
    "sys.path.append('Src/')\n",
    "from data_modificado import * #hay funciones que estan cambiadas en este script para adaptralas a nuestro dataset\n",
    "from train_2 import * #este hubo que modificar una linea tambien\n",
    "from transfer_learning import * #hubo que modificart lo mismo que en train_2\n",
    "from test_functions import *\n",
    "from layers import *\n",
    "from utils import *\n",
    "from loss import *\n",
    "from metric import *\n",
    "from results import *\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "import itertools as it\n",
    "\n",
    "#funciones\n",
    "def read_df(\n",
    "              metadata_names=['age','Temperature','Precipitation3Days'],\n",
    "              random_state=42,\n",
    "              otu_filename='../Datasets/otu_table_all_80.csv',\n",
    "              metadata_filename='../Datasets/metadata_table_all_80.csv'):\n",
    "    otu = pd.read_csv(otu_filename, index_col=0, header=None).T\n",
    "    #print(otu.head())\n",
    "    otu = otu.set_index('otuids')\n",
    "    otu = otu.astype('int32')\n",
    "    metadata = pd.read_csv(metadata_filename)\n",
    "    #print(metadata.head())\n",
    "    metadata = metadata.set_index('X.SampleID')\n",
    "    metadata.head()\n",
    "    domain = metadata[metadata_names]\n",
    "    #if 'INBREDS' in metadata_names:\n",
    "    #    domain = pd.concat([domain, pd.get_dummies(domain['INBREDS'], prefix='INBREDS')], axis=1)\n",
    "    #    domain = domain.drop(['INBREDS'], axis=1)\n",
    "    #elif 'Maize_Line' in metadata_names:\n",
    "    #    domain = pd.concat([domain, pd.get_dummies(domain['Maize_Line'], prefix='Maize_Line')], axis=1)\n",
    "    #    domain = domain.drop(['Maize_Line'], axis=1) \n",
    "    df = pd.concat([otu, domain], axis=1, sort=True, join='outer')\n",
    "    #print(df.head())\n",
    "    #data_microbioma = df[otu.columns].to_numpy(dtype=np.float32)\n",
    "    #data_domain = df[domain.columns].to_numpy(dtype=np.float32)\n",
    "    df_microbioma = df[otu.columns]\n",
    "    df_domain = df[domain.columns]\n",
    "    df_domain.head()\n",
    "    df_microbioma_train, df_microbioma_no_train, df_domain_train, df_domain_no_train = \\\n",
    "        train_test_split(df_microbioma, df_domain, test_size=0.1, random_state=random_state)\n",
    "    # Transfer learning subset\n",
    "    df_microbioma_test, df_microbioma_transfer_learning, df_domain_test, df_domain_transfer_learning = \\\n",
    "        train_test_split(df_microbioma_no_train, df_domain_no_train, test_size=0.1, random_state=random_state)\n",
    "    df_microbioma_transfer_learning_train, df_microbioma_transfer_learning_test, df_domain_transfer_learning_train, df_domain_transfer_learning_test = \\\n",
    "        train_test_split(df_microbioma_transfer_learning, df_domain_transfer_learning, test_size=0.3, random_state=random_state)\n",
    "    \n",
    "    return df_microbioma_train, df_microbioma_test, df_microbioma_transfer_learning_train, df_microbioma_transfer_learning_test, df_domain_train, df_domain_test, df_domain_transfer_learning_train, df_domain_transfer_learning_test, otu.columns, domain.columns\n",
    "    #return df_microbioma_train, df_microbioma_test, df_microbioma_transfer_learning_train, df_microbioma_transfer_learning_test, df_domain_train, df_domain_test, df_domain_transfer_learning_train, df_domain_transfer_learning_test, otu.columns, domain.columns\n",
    "\n",
    "\n",
    "def train_kfold_mod(model_fn, m_train, d_train, z_train, m_test, d_test, z_test,\n",
    "                batch_size, epochs, train_callbacks):\n",
    "    all_models = model_fn()\n",
    "    model, encoder_bioma, encoder_domain, decoder_bioma = all_models\n",
    "    metrics_prefix = None\n",
    "    if encoder_bioma is not None and encoder_domain is not None:\n",
    "        x_train = (m_train, d_train)\n",
    "        y_train = (m_train, m_train, z_train)\n",
    "        x_test = (m_test, d_test)\n",
    "        y_test = (m_test, m_test, z_test)\n",
    "    elif encoder_bioma is not None:\n",
    "        x_train = m_train\n",
    "        y_train = m_train\n",
    "        x_test = m_test\n",
    "        y_test = m_test\n",
    "        metrics_prefix = 'bioma'\n",
    "    elif encoder_domain is not None:\n",
    "        x_train = d_train\n",
    "        y_train = m_train\n",
    "        x_test = d_test\n",
    "        y_test = m_test\n",
    "        metrics_prefix = 'domain'\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(5000).batch(\n",
    "        batch_size)\n",
    "    train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "    val_dataset = val_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    r = model.fit(train_dataset,\n",
    "                  epochs=epochs,\n",
    "                  validation_data=val_dataset,\n",
    "                  callbacks=train_callbacks,\n",
    "                  verbose=0)\n",
    "    if metrics_prefix is not None:\n",
    "        old_keys = r.history\n",
    "        r.history = {}\n",
    "        for k, v in old_keys.items():\n",
    "            if k == 'loss' or k == 'val_loss':\n",
    "                new_key = k\n",
    "            elif k.startswith('val_'):\n",
    "                new_key = 'val_{}_{}'.format(metrics_prefix, k[4:])\n",
    "            else:\n",
    "                new_key = '{}_{}'.format(metrics_prefix, k)\n",
    "            r.history[new_key] = v\n",
    "    del val_dataset\n",
    "    del train_dataset\n",
    "    del x_train\n",
    "    del y_train\n",
    "    del x_test\n",
    "    del y_test\n",
    "    return r, all_models\n",
    "\n",
    "def train_2(model_fn,\n",
    "          data_microbioma,\n",
    "          data_domain,\n",
    "          latent_space=10,\n",
    "          folds=5,\n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          learning_rate_scheduler=ExpDecayScheluder(),\n",
    "          random_seed=347,\n",
    "          verbose=0):\n",
    "    data_zeros_latent = np.zeros((data_microbioma.shape[0], latent_space), dtype=data_microbioma.dtype)\n",
    "    results = []\n",
    "    models = []\n",
    "    train_callbacks = [\n",
    "        callbacks.EarlyStopping(monitor='val_loss', patience=epochs + 1, restore_best_weights=True)]\n",
    "    if verbose >= 0:\n",
    "        train_callbacks += [TqdmCallback(verbose=verbose)]\n",
    "    if learning_rate_scheduler is not None:\n",
    "        train_callbacks += [learning_rate_scheduler.make()]\n",
    "\n",
    "    if folds <= 1:\n",
    "        m_train, m_test = data_microbioma, data_microbioma\n",
    "        d_train, d_test = data_domain, data_domain\n",
    "        z_train, z_test = data_zeros_latent, data_zeros_latent\n",
    "        tf.random.set_seed(random_seed)\n",
    "        r, m = train_kfold(model_fn, m_train, d_train, z_train, m_test, d_test, z_test,\n",
    "                           batch_size, epochs, train_callbacks)\n",
    "        results.append(r)\n",
    "        models.append(m)\n",
    "\n",
    "    else: #EL PROBLEMA ESTA AQUI, QUE HACE FALTA UN \n",
    "        kf = KFold(n_splits=folds, random_state=random_seed, shuffle=True)\n",
    "        tf.random.set_seed(random_seed)\n",
    "\n",
    "        for train_index, test_index in kf.split(data_microbioma):\n",
    "            m_train, m_test = data_microbioma[train_index], data_microbioma[test_index]\n",
    "            #print(m_train)\n",
    "            #d_train, d_test = data_domain[train_index], data_domain[test_index]\n",
    "            if data_domain is None:\n",
    "                d_train, d_test = None, None\n",
    "            else:\n",
    "                d_train, d_test = data_domain[train_index], data_domain[test_index]\n",
    "            #print(d_train)\n",
    "            #Esto de hacer el if else ha funcionado, pero no se si hace lo que debe bien\n",
    "            z_train, z_test = data_zeros_latent[train_index], data_zeros_latent[test_index]\n",
    "            r, m = train_kfold_mod(model_fn, m_train, d_train, z_train, m_test, d_test, z_test,\n",
    "                               batch_size, epochs, train_callbacks)\n",
    "            results.append(r)\n",
    "            models.append(m)\n",
    "    return results, models\n",
    "\n",
    "def perform_experiment_2_mod(cv_folds, epochs, batch_size, learning_rate, optimizer,\n",
    "                       learning_rate_scheduler, input_transform, output_transform,\n",
    "                       reconstruction_loss, latent_space, layers,\n",
    "                       activation, activation_latent,\n",
    "                       data_microbioma_train, data_domain_train,\n",
    "                       show_results=True, device='/CPU:0'): #Show results cambiado de False  aTrue\n",
    "    if input_transform is not None:\n",
    "        input_transform = input_transform()\n",
    "    #----------    \n",
    "    if output_transform is not None:\n",
    "        output_transform = output_transform()\n",
    "    #----------      \n",
    "    if reconstruction_loss.__class__.__name__ == 'MakeLoss':\n",
    "        reconstruction_loss = reconstruction_loss.make()\n",
    "    else:\n",
    "        reconstruction_loss = reconstruction_loss()\n",
    "    domain_layers = [l // 16 for l in layers] ####que es esto???? Esto es para las capas del domain\n",
    "    #print(domain_layers)\n",
    "    bioma_autoencoder = \" -> \".join([\"b\"] +\n",
    "                                    [str(l) for l in layers] +\n",
    "                                    [str(latent_space)] +\n",
    "                                    [str(l) for l in reversed(layers)] +\n",
    "                                    [\"b\"])\n",
    "    #---------- \n",
    "    #esto solo se utiliza para el texto, es irrelevante para nuestro error\n",
    "    if data_domain_train is not None:\n",
    "        domain_autoencoder = \" -> \".join([\"d\"] +\n",
    "                                     [str(l) for l in domain_layers] +\n",
    "                                     [str(latent_space)] +\n",
    "                                     [str(l) for l in reversed(layers)] +\n",
    "                                     [\"b\"])\n",
    "        \n",
    "    else: \n",
    "        domain_autoencoder = \" \"\n",
    "    #---------- \n",
    "    #donde se usa domain autoencoder?\n",
    "    in_transform_name = input_transform.__class__.__name__ if input_transform else \"none\"\n",
    "    out_transform_name = output_transform.__class__.__name__ if output_transform else \"none\"\n",
    "    lr_scheduler_text = learning_rate_scheduler[\n",
    "        1] if learning_rate_scheduler is not None else \"none\"\n",
    "    lr_text = learning_rate if learning_rate_scheduler is not None else \"constant = {}\".format(\n",
    "        learning_rate)\n",
    "    learning_rate_scheduler = learning_rate_scheduler[\n",
    "        0] if learning_rate_scheduler is not None else None\n",
    "    optimizer = optimizer(learning_rate=learning_rate)\n",
    "    #---------- \n",
    "    experiment_parameters = [\n",
    "        (\"Input transform\", in_transform_name),\n",
    "        (\"Output transform\", out_transform_name),\n",
    "        (\"Reconstruction Loss\", reconstruction_loss.__class__.__name__),\n",
    "        (\"Latent Space\", latent_space),\n",
    "        (\"Bioma Autoencoder\", bioma_autoencoder),\n",
    "        (\"Domain Autoencoder\", domain_autoencoder),\n",
    "        (\"Activation Encoder\", activation),\n",
    "        (\"Activation Decoder\", activation),\n",
    "        (\"Activation Latent\", activation_latent),\n",
    "        (\"CV folds\", cv_folds),\n",
    "        (\"Epochs\", epochs),\n",
    "        (\"Batch Size\", batch_size),\n",
    "        (\"Learning Rate Scheduler\", lr_scheduler_text),\n",
    "        (\"Learning Rate\", lr_text),\n",
    "        (\"Optimizer\", optimizer.__class__.__name__),\n",
    "    ]\n",
    "    #----------  \n",
    "    if show_results:\n",
    "        md_text = \"\"\n",
    "        md_text += \"| Parameter             | Value         |\\n\"\n",
    "        md_text += \"|:----------------------|:--------------|\\n\"\n",
    "        for n, v in experiment_parameters:\n",
    "            md_text += \"| {} | {} |\\n\".format(n, v)\n",
    "\n",
    "        display(Markdown(md_text))\n",
    "    #------------\n",
    "    def create_model(print_data=False):\n",
    "        bioma_shape=data_microbioma_train.shape[1]\n",
    "        \n",
    "        if data_domain_train is not None:\n",
    "            domain_shape=data_domain_train.shape[1]\n",
    "            #print(\"data_domain_train!=None\")\n",
    "        else:\n",
    "            domain_shape=None\n",
    "            #print(\"data_domain_train==None\")\n",
    "        models = autoencoder(bioma_shape=bioma_shape,\n",
    "                             #bioma_shape=717,\n",
    "                             domain_shape=domain_shape,\n",
    "                             output_shape=bioma_shape,\n",
    "                             #output_shape=717,\n",
    "                             latent_space=latent_space,\n",
    "                             bioma_layers=layers, #Esto es lo de [512,316]\n",
    "                             domain_layers=domain_layers, #Esto son cada una de las layers divididas por 16\n",
    "                             input_transform=input_transform,\n",
    "                             output_transform=output_transform,\n",
    "                             activation_function_encoder=activation,\n",
    "                             activation_function_decoder=activation,\n",
    "                             activation_function_latent=activation_latent)\n",
    "        #Entiendo analizando lo demas que aqui NO esta el error\n",
    "        #la funcion autoencoder esta en model.py (es la unica funcion en ese script)\n",
    "        \n",
    "        model, encoder_bioma, encoder_domain, decoder_bioma = models\n",
    "\n",
    "        if print_data:\n",
    "            plot_models(model, encoder_bioma, encoder_domain, decoder_bioma)\n",
    "        compile_train(model,\n",
    "                      encoder_bioma=encoder_bioma,\n",
    "                      encoder_domain=encoder_domain,\n",
    "                      reconstruction_error=reconstruction_loss,\n",
    "                      encoded_comparison_error=losses.MeanAbsoluteError(),\n",
    "                      metrics=get_experiment_metrics(input_transform, output_transform),\n",
    "                      optimizer=optimizer)\n",
    "        \n",
    "        #print(\"He acabado create_model :)\")\n",
    "        return model, encoder_bioma, encoder_domain, decoder_bioma\n",
    "    #-----------\n",
    "    create_model(print_data=False)\n",
    "    #-----------\n",
    "    #Esta en esta seccion el problema, en train_2\n",
    "    #print(data_domain_train)\n",
    "    #print(latent_space)\n",
    "    with tf.device(device):\n",
    "        results, models = train_2(create_model,\n",
    "                                data_microbioma_train,\n",
    "                                data_domain_train,\n",
    "                                latent_space=latent_space,\n",
    "                                folds=cv_folds,\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                learning_rate_scheduler=learning_rate_scheduler,\n",
    "                                verbose=-1)\n",
    "    #----------\n",
    "    validation_results = print_results(results, show_results=show_results)\n",
    "    if show_results:\n",
    "        display(Markdown(\"*************\"))\n",
    "\n",
    "    return experiment_parameters + validation_results, models, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-timer",
   "metadata": {},
   "source": [
    "__params:__\n",
    " - \"activat_func\":[\"softmax\",\"sigmoid\",\"relu\",\"tanh\"]\n",
    " - \"activ_ouput\":[\"softmax\",\"sigmoid\",\"relu\",\"tanh\"]\n",
    " - \"learning_rate\":[0.01,0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "optimum-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos los datos\n",
    "nombres_metadatos = [\"KCAL\",\"PROT\",\"TFAT\",\"CARB\",\"MOIS\",\"ALC\",\"CAFF\",\"THEO\",\"SUGR\",\"FIBE\",\"CALC\",\"IRON\",\"MAGN\",\"PHOS\",\"POTA\",\"SODI\",\"ZINC\",\"COPP\",\"SELE\",\"VC\",\"VB1\",\"VB2\",\"NIAC\",\"VB6\",\"FOLA\",\"VB12\",\"VARA\",\"RET\",\"BCAR\",\"ACAR\",\"CRYP\",\"LYCO\",\"LZ\",\"ATOC\",\"VK\",\"CHOLE\",\"SFAT\",\"MFAT\",\"PFAT\",\"VITD\",\"CHOLN\"]\n",
    "#nombres_metadatos = [\"PROT\",\"TFAT\",\"CARB\",\"MOIS\",\"ALC\",\"CAFF\",\"THEO\",\"CALC\",\"MAGN\",\"POTA\",\"ZINC\",\"VC\",\"VB1\",\"VB6\",\"VARA\",\"ACAR\",\"CRYP\",\"LYCO\",\"ATOC\",\"VK\",\"CHOLE\",\"VITD\"]\n",
    "df_microbioma_train, df_microbioma_test, _, _, \\\n",
    "df_domain_train, df_domain_test, _, _, otu_columns, domain_columns = read_df(metadata_names=nombres_metadatos,otu_filename='resultados_ana/datos_otus_filtrados/otu_table_especies_80.csv',metadata_filename='resultados_ana/metadatos_nutrientes.csv')\n",
    "\n",
    "data_microbioma_train = df_microbioma_train.to_numpy(dtype=np.float32)\n",
    "data_microbioma_test = df_microbioma_test.to_numpy(dtype=np.float32)\n",
    "data_domain_train = df_domain_train.to_numpy(dtype=np.float32)\n",
    "data_domain_test = df_domain_test.to_numpy(dtype=np.float32)\n",
    "\n",
    "#Preparamos las combinaciones pertinentes (5 mejores)\n",
    "combinations = [[100,64,0.001,optimizers.Adam,15,[512,256],\"tanh\",\"tanh\"],\\\n",
    "                [100,96,0.001,optimizers.Adam,15,[512,256],\"tanh\",\"tanh\"],\\\n",
    "                [100,64,0.01,optimizers.Adam,15,[512,256],\"relu\",\"tanh\"],\\\n",
    "                [100,96,0.001,optimizers.Adam,15,[512,256,128],\"tanh\",\"tanh\"],\\\n",
    "                [100,64,0.001,optimizers.Adam,15,[512,256,128],\"tanh\",\"relu\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-delay",
   "metadata": {},
   "source": [
    "### 0.001, tanh, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "referenced-robin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5198239684104919 | 0.5198239684104919 | 0.5198239684104919 |\n",
       "| pearson_corr | 0.5679537057876587 | 0.5679537057876587 | 0.5679537057876587 |\n",
       "| jensen_shannon_divergence | 1.4208630323410034 | 1.4208630323410034 | 1.4208630323410034 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5278350114822388 | 0.5278350114822388 | 0.5278350114822388 |\n",
       "| pearson_corr | 0.5342879891395569 | 0.5342879891395569 | 0.5342879891395569 |\n",
       "| jensen_shannon_divergence | 1.4983304738998413 | 1.4983304738998413 | 1.4983304738998413 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.49953192472457886 | 0.49953192472457886 | 0.49953192472457886 |\n",
       "| pearson_corr | 0.5958179235458374 | 0.5958179235458374 | 0.5958179235458374 |\n",
       "| jensen_shannon_divergence | 1.5064880847930908 | 1.5064880847930908 | 1.5064880847930908 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47971343994140625 | 0.47971343994140625 | 0.47971343994140625 |\n",
       "| pearson_corr | 0.6003298163414001 | 0.6003298163414001 | 0.6003298163414001 |\n",
       "| jensen_shannon_divergence | 1.3072468042373657 | 1.3072468042373657 | 1.3072468042373657 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5127285718917847 | 0.5127285718917847 | 0.5127285718917847 |\n",
       "| pearson_corr | 0.5599632859230042 | 0.5599632859230042 | 0.5599632859230042 |\n",
       "| jensen_shannon_divergence | 1.4083970785140991 | 1.4083970785140991 | 1.4083970785140991 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation=None)(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-development",
   "metadata": {},
   "source": [
    "### 0.001, tanh, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accomplished-folks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5999472737312317 | 0.5999472737312317 | 0.5999472737312317 |\n",
       "| pearson_corr | 0.5601440668106079 | 0.5601440668106079 | 0.5601440668106079 |\n",
       "| jensen_shannon_divergence | 1.6068118810653687 | 1.6068118810653687 | 1.6068118810653687 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6230741739273071 | 0.6230741739273071 | 0.6230741739273071 |\n",
       "| pearson_corr | 0.5433692932128906 | 0.5433692932128906 | 0.5433692932128906 |\n",
       "| jensen_shannon_divergence | 1.7349467277526855 | 1.7349467277526855 | 1.7349467277526855 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5667614340782166 | 0.5667614340782166 | 0.5667614340782166 |\n",
       "| pearson_corr | 0.5106623768806458 | 0.5106623768806458 | 0.5106623768806458 |\n",
       "| jensen_shannon_divergence | 1.7916470766067505 | 1.7916470766067505 | 1.7916470766067505 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.61825031042099 | 0.61825031042099 | 0.61825031042099 |\n",
       "| pearson_corr | 0.4273053705692291 | 0.4273053705692291 | 0.4273053705692291 |\n",
       "| jensen_shannon_divergence | 1.7056517601013184 | 1.7056517601013184 | 1.7056517601013184 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6380680799484253 | 0.6380680799484253 | 0.6380680799484253 |\n",
       "| pearson_corr | 0.48751795291900635 | 0.48751795291900635 | 0.48751795291900635 |\n",
       "| jensen_shannon_divergence | 1.872589111328125 | 1.872589111328125 | 1.872589111328125 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-airport",
   "metadata": {},
   "source": [
    "### 0.001, tanh, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "electoral-speaking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5348986983299255 | 0.5348986983299255 | 0.5348986983299255 |\n",
       "| pearson_corr | 0.5786547660827637 | 0.5786547660827637 | 0.5786547660827637 |\n",
       "| jensen_shannon_divergence | 1.32582688331604 | 1.32582688331604 | 1.32582688331604 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.55926913022995 | 0.55926913022995 | 0.55926913022995 |\n",
       "| pearson_corr | 0.577221155166626 | 0.577221155166626 | 0.577221155166626 |\n",
       "| jensen_shannon_divergence | 1.3999131917953491 | 1.3999131917953491 | 1.3999131917953491 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5632506608963013 | 0.5632506608963013 | 0.5632506608963013 |\n",
       "| pearson_corr | 0.5310698747634888 | 0.5310698747634888 | 0.5310698747634888 |\n",
       "| jensen_shannon_divergence | 1.7271572351455688 | 1.7271572351455688 | 1.7271572351455688 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6004998087882996 | 0.6004998087882996 | 0.6004998087882996 |\n",
       "| pearson_corr | 0.3899085819721222 | 0.3899085819721222 | 0.3899085819721222 |\n",
       "| jensen_shannon_divergence | 1.6396328210830688 | 1.6396328210830688 | 1.6396328210830688 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4868283271789551 | 0.4868283271789551 | 0.4868283271789551 |\n",
       "| pearson_corr | 0.6170819997787476 | 0.6170819997787476 | 0.6170819997787476 |\n",
       "| jensen_shannon_divergence | 1.2850221395492554 | 1.2850221395492554 | 1.2850221395492554 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-tradition",
   "metadata": {},
   "source": [
    "### 0.001, tanh, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "progressive-soundtrack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5435793995857239 | 0.5435793995857239 | 0.5435793995857239 |\n",
       "| pearson_corr | 0.5569123029708862 | 0.5569123029708862 | 0.5569123029708862 |\n",
       "| jensen_shannon_divergence | 1.394425868988037 | 1.394425868988037 | 1.394425868988037 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5691697001457214 | 0.5691697001457214 | 0.5691697001457214 |\n",
       "| pearson_corr | 0.5321919322013855 | 0.5321919322013855 | 0.5321919322013855 |\n",
       "| jensen_shannon_divergence | 1.4524837732315063 | 1.4524837732315063 | 1.4524837732315063 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.537052571773529 | 0.537052571773529 | 0.537052571773529 |\n",
       "| pearson_corr | 0.5711638927459717 | 0.5711638927459717 | 0.5711638927459717 |\n",
       "| jensen_shannon_divergence | 1.6124883890151978 | 1.6124883890151978 | 1.6124883890151978 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6134063005447388 | 0.6134063005447388 | 0.6134063005447388 |\n",
       "| pearson_corr | 0.35309308767318726 | 0.35309308767318726 | 0.35309308767318726 |\n",
       "| jensen_shannon_divergence | 1.7744197845458984 | 1.7744197845458984 | 1.7744197845458984 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4707522690296173 | 0.4707522690296173 | 0.4707522690296173 |\n",
       "| pearson_corr | 0.6295716762542725 | 0.6295716762542725 | 0.6295716762542725 |\n",
       "| jensen_shannon_divergence | 1.3059877157211304 | 1.3059877157211304 | 1.3059877157211304 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olympic-closer",
   "metadata": {},
   "source": [
    "### 0.001, softmax, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "prerequisite-accent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47490984201431274 | 0.47490984201431274 | 0.47490984201431274 |\n",
       "| pearson_corr | 0.6169838309288025 | 0.6169838309288025 | 0.6169838309288025 |\n",
       "| jensen_shannon_divergence | 1.2010362148284912 | 1.2010362148284912 | 1.2010362148284912 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47951531410217285 | 0.47951531410217285 | 0.47951531410217285 |\n",
       "| pearson_corr | 0.6239034533500671 | 0.6239034533500671 | 0.6239034533500671 |\n",
       "| jensen_shannon_divergence | 1.20426607131958 | 1.20426607131958 | 1.20426607131958 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4922023415565491 | 0.4922023415565491 | 0.4922023415565491 |\n",
       "| pearson_corr | 0.632620632648468 | 0.632620632648468 | 0.632620632648468 |\n",
       "| jensen_shannon_divergence | 1.3148760795593262 | 1.3148760795593262 | 1.3148760795593262 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.48565179109573364 | 0.48565179109573364 | 0.48565179109573364 |\n",
       "| pearson_corr | 0.6147162318229675 | 0.6147162318229675 | 0.6147162318229675 |\n",
       "| jensen_shannon_divergence | 1.2542065382003784 | 1.2542065382003784 | 1.2542065382003784 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.48159801959991455 | 0.48159801959991455 | 0.48159801959991455 |\n",
       "| pearson_corr | 0.6157978177070618 | 0.6157978177070618 | 0.6157978177070618 |\n",
       "| jensen_shannon_divergence | 1.3004523515701294 | 1.3004523515701294 | 1.3004523515701294 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "  \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-meditation",
   "metadata": {},
   "source": [
    "### 0.001, softmax, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "competent-italic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5873152613639832 | 0.5873152613639832 | 0.5873152613639832 |\n",
       "| pearson_corr | 0.5695202350616455 | 0.5695202350616455 | 0.5695202350616455 |\n",
       "| jensen_shannon_divergence | 1.5207065343856812 | 1.5207065343856812 | 1.5207065343856812 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6173149943351746 | 0.6173149943351746 | 0.6173149943351746 |\n",
       "| pearson_corr | 0.5581154227256775 | 0.5581154227256775 | 0.5581154227256775 |\n",
       "| jensen_shannon_divergence | 1.6747305393218994 | 1.6747305393218994 | 1.6747305393218994 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5519635081291199 | 0.5519635081291199 | 0.5519635081291199 |\n",
       "| pearson_corr | 0.5478900074958801 | 0.5478900074958801 | 0.5478900074958801 |\n",
       "| jensen_shannon_divergence | 1.6595872640609741 | 1.6595872640609741 | 1.6595872640609741 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.613609790802002 | 0.613609790802002 | 0.613609790802002 |\n",
       "| pearson_corr | 0.4258407950401306 | 0.4258407950401306 | 0.4258407950401306 |\n",
       "| jensen_shannon_divergence | 1.6335529088974 | 1.6335529088974 | 1.6335529088974 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6449507474899292 | 0.6449507474899292 | 0.6449507474899292 |\n",
       "| pearson_corr | 0.49679017066955566 | 0.49679017066955566 | 0.49679017066955566 |\n",
       "| jensen_shannon_divergence | 1.870395541191101 | 1.870395541191101 | 1.870395541191101 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "\n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-building",
   "metadata": {},
   "source": [
    "### 0.001, softmax, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "changing-swift",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5424107313156128 | 0.5424107313156128 | 0.5424107313156128 |\n",
       "| pearson_corr | 0.5868514180183411 | 0.5868514180183411 | 0.5868514180183411 |\n",
       "| jensen_shannon_divergence | 1.3533656597137451 | 1.3533656597137451 | 1.3533656597137451 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.567302405834198 | 0.567302405834198 | 0.567302405834198 |\n",
       "| pearson_corr | 0.5836503505706787 | 0.5836503505706787 | 0.5836503505706787 |\n",
       "| jensen_shannon_divergence | 1.4511511325836182 | 1.4511511325836182 | 1.4511511325836182 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5442705750465393 | 0.5442705750465393 | 0.5442705750465393 |\n",
       "| pearson_corr | 0.5774242877960205 | 0.5774242877960205 | 0.5774242877960205 |\n",
       "| jensen_shannon_divergence | 1.5786257982254028 | 1.5786257982254028 | 1.5786257982254028 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6222888231277466 | 0.6222888231277466 | 0.6222888231277466 |\n",
       "| pearson_corr | 0.3455978035926819 | 0.3455978035926819 | 0.3455978035926819 |\n",
       "| jensen_shannon_divergence | 1.7254823446273804 | 1.7254823446273804 | 1.7254823446273804 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.48138856887817383 | 0.48138856887817383 | 0.48138856887817383 |\n",
       "| pearson_corr | 0.6163668632507324 | 0.6163668632507324 | 0.6163668632507324 |\n",
       "| jensen_shannon_divergence | 1.2993035316467285 | 1.2993035316467285 | 1.2993035316467285 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "\n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-rental",
   "metadata": {},
   "source": [
    "### 0.001, softmax, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "virtual-chicken",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5407777428627014 | 0.5407777428627014 | 0.5407777428627014 |\n",
       "| pearson_corr | 0.586143434047699 | 0.586143434047699 | 0.586143434047699 |\n",
       "| jensen_shannon_divergence | 1.3346744775772095 | 1.3346744775772095 | 1.3346744775772095 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.561308741569519 | 0.561308741569519 | 0.561308741569519 |\n",
       "| pearson_corr | 0.5853216648101807 | 0.5853216648101807 | 0.5853216648101807 |\n",
       "| jensen_shannon_divergence | 1.4094852209091187 | 1.4094852209091187 | 1.4094852209091187 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5677441358566284 | 0.5677441358566284 | 0.5677441358566284 |\n",
       "| pearson_corr | 0.5174671411514282 | 0.5174671411514282 | 0.5174671411514282 |\n",
       "| jensen_shannon_divergence | 1.7296377420425415 | 1.7296377420425415 | 1.7296377420425415 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6129594445228577 | 0.6129594445228577 | 0.6129594445228577 |\n",
       "| pearson_corr | 0.4099173843860626 | 0.4099173843860626 | 0.4099173843860626 |\n",
       "| jensen_shannon_divergence | 1.7117807865142822 | 1.7117807865142822 | 1.7117807865142822 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.48678046464920044 | 0.48678046464920044 | 0.48678046464920044 |\n",
       "| pearson_corr | 0.6144310832023621 | 0.6144310832023621 | 0.6144310832023621 |\n",
       "| jensen_shannon_divergence | 1.3378607034683228 | 1.3378607034683228 | 1.3378607034683228 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "\n",
    "\n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-egyptian",
   "metadata": {},
   "source": [
    "### 0.001, sigmoid, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bibliographic-silly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4591425955295563 | 0.4591425955295563 | 0.4591425955295563 |\n",
       "| pearson_corr | 0.6451307535171509 | 0.6451307535171509 | 0.6451307535171509 |\n",
       "| jensen_shannon_divergence | 1.1471294164657593 | 1.1471294164657593 | 1.1471294164657593 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.46835750341415405 | 0.46835750341415405 | 0.46835750341415405 |\n",
       "| pearson_corr | 0.638234555721283 | 0.638234555721283 | 0.638234555721283 |\n",
       "| jensen_shannon_divergence | 1.165932059288025 | 1.165932059288025 | 1.165932059288025 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.48977357149124146 | 0.48977357149124146 | 0.48977357149124146 |\n",
       "| pearson_corr | 0.6309012174606323 | 0.6309012174606323 | 0.6309012174606323 |\n",
       "| jensen_shannon_divergence | 1.359602451324463 | 1.359602451324463 | 1.359602451324463 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4685311019420624 | 0.4685311019420624 | 0.4685311019420624 |\n",
       "| pearson_corr | 0.6424252986907959 | 0.6424252986907959 | 0.6424252986907959 |\n",
       "| jensen_shannon_divergence | 1.2007874250411987 | 1.2007874250411987 | 1.2007874250411987 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47043296694755554 | 0.47043296694755554 | 0.47043296694755554 |\n",
       "| pearson_corr | 0.6313848495483398 | 0.6313848495483398 | 0.6313848495483398 |\n",
       "| jensen_shannon_divergence | 1.238441824913025 | 1.238441824913025 | 1.238441824913025 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "\n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-designer",
   "metadata": {},
   "source": [
    "### 0.001, sigmoid, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "serial-mustang",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5883068442344666 | 0.5883068442344666 | 0.5883068442344666 |\n",
       "| pearson_corr | 0.5728752613067627 | 0.5728752613067627 | 0.5728752613067627 |\n",
       "| jensen_shannon_divergence | 1.537861943244934 | 1.537861943244934 | 1.537861943244934 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.614381730556488 | 0.614381730556488 | 0.614381730556488 |\n",
       "| pearson_corr | 0.5612325668334961 | 0.5612325668334961 | 0.5612325668334961 |\n",
       "| jensen_shannon_divergence | 1.6650081872940063 | 1.6650081872940063 | 1.6650081872940063 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5540395975112915 | 0.5540395975112915 | 0.5540395975112915 |\n",
       "| pearson_corr | 0.5446517467498779 | 0.5446517467498779 | 0.5446517467498779 |\n",
       "| jensen_shannon_divergence | 1.6753579378128052 | 1.6753579378128052 | 1.6753579378128052 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6147810220718384 | 0.6147810220718384 | 0.6147810220718384 |\n",
       "| pearson_corr | 0.42673179507255554 | 0.42673179507255554 | 0.42673179507255554 |\n",
       "| jensen_shannon_divergence | 1.6408389806747437 | 1.6408389806747437 | 1.6408389806747437 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6405277848243713 | 0.6405277848243713 | 0.6405277848243713 |\n",
       "| pearson_corr | 0.5067748427391052 | 0.5067748427391052 | 0.5067748427391052 |\n",
       "| jensen_shannon_divergence | 1.856258749961853 | 1.856258749961853 | 1.856258749961853 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "\n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-compound",
   "metadata": {},
   "source": [
    "### 0.001, sigmoid, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "liberal-packet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.541381299495697 | 0.541381299495697 | 0.541381299495697 |\n",
       "| pearson_corr | 0.5917224287986755 | 0.5917224287986755 | 0.5917224287986755 |\n",
       "| jensen_shannon_divergence | 1.3473080396652222 | 1.3473080396652222 | 1.3473080396652222 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5644711256027222 | 0.5644711256027222 | 0.5644711256027222 |\n",
       "| pearson_corr | 0.587166965007782 | 0.587166965007782 | 0.587166965007782 |\n",
       "| jensen_shannon_divergence | 1.4332478046417236 | 1.4332478046417236 | 1.4332478046417236 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5396001935005188 | 0.5396001935005188 | 0.5396001935005188 |\n",
       "| pearson_corr | 0.5866538286209106 | 0.5866538286209106 | 0.5866538286209106 |\n",
       "| jensen_shannon_divergence | 1.5406091213226318 | 1.5406091213226318 | 1.5406091213226318 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6149333119392395 | 0.6149333119392395 | 0.6149333119392395 |\n",
       "| pearson_corr | 0.3657122552394867 | 0.3657122552394867 | 0.3657122552394867 |\n",
       "| jensen_shannon_divergence | 1.6930550336837769 | 1.6930550336837769 | 1.6930550336837769 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47176235914230347 | 0.47176235914230347 | 0.47176235914230347 |\n",
       "| pearson_corr | 0.6304966807365417 | 0.6304966807365417 | 0.6304966807365417 |\n",
       "| jensen_shannon_divergence | 1.2556648254394531 | 1.2556648254394531 | 1.2556648254394531 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "\n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-musician",
   "metadata": {},
   "source": [
    "### 0.001, sigmoid, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "undefined-actress",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5445992350578308 | 0.5445992350578308 | 0.5445992350578308 |\n",
       "| pearson_corr | 0.5686149001121521 | 0.5686149001121521 | 0.5686149001121521 |\n",
       "| jensen_shannon_divergence | 1.3432663679122925 | 1.3432663679122925 | 1.3432663679122925 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5568971633911133 | 0.5568971633911133 | 0.5568971633911133 |\n",
       "| pearson_corr | 0.5756428837776184 | 0.5756428837776184 | 0.5756428837776184 |\n",
       "| jensen_shannon_divergence | 1.3961070775985718 | 1.3961070775985718 | 1.3961070775985718 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5818531513214111 | 0.5818531513214111 | 0.5818531513214111 |\n",
       "| pearson_corr | 0.4856674373149872 | 0.4856674373149872 | 0.4856674373149872 |\n",
       "| jensen_shannon_divergence | 1.8313473463058472 | 1.8313473463058472 | 1.8313473463058472 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6184325814247131 | 0.6184325814247131 | 0.6184325814247131 |\n",
       "| pearson_corr | 0.39855676889419556 | 0.39855676889419556 | 0.39855676889419556 |\n",
       "| jensen_shannon_divergence | 1.7551757097244263 | 1.7551757097244263 | 1.7551757097244263 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5028839707374573 | 0.5028839707374573 | 0.5028839707374573 |\n",
       "| pearson_corr | 0.5854948163032532 | 0.5854948163032532 | 0.5854948163032532 |\n",
       "| jensen_shannon_divergence | 1.3412530422210693 | 1.3412530422210693 | 1.3412530422210693 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-hacker",
   "metadata": {},
   "source": [
    "### 0.001, relu, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "mobile-fitness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.586635947227478 | 0.586635947227478 | 0.586635947227478 |\n",
       "| pearson_corr | 0.5783306956291199 | 0.5783306956291199 | 0.5783306956291199 |\n",
       "| jensen_shannon_divergence | 2.4035918712615967 | 2.4035918712615967 | 2.4035918712615967 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7052851915359497 | 0.7052851915359497 | 0.7052851915359497 |\n",
       "| pearson_corr | 0.39580652117729187 | 0.39580652117729187 | 0.39580652117729187 |\n",
       "| jensen_shannon_divergence | 2.8427534103393555 | 2.8427534103393555 | 2.8427534103393555 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7925626635551453 | 0.7925626635551453 | 0.7925626635551453 |\n",
       "| pearson_corr | 0.1745051145553589 | 0.1745051145553589 | 0.1745051145553589 |\n",
       "| jensen_shannon_divergence | 3.4136650562286377 | 3.4136650562286377 | 3.4136650562286377 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6730049848556519 | 0.6730049848556519 | 0.6730049848556519 |\n",
       "| pearson_corr | 0.32157495617866516 | 0.32157495617866516 | 0.32157495617866516 |\n",
       "| jensen_shannon_divergence | 2.434028148651123 | 2.434028148651123 | 2.434028148651123 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6909385919570923 | 0.6909385919570923 | 0.6909385919570923 |\n",
       "| pearson_corr | 0.2342325747013092 | 0.2342325747013092 | 0.2342325747013092 |\n",
       "| jensen_shannon_divergence | 2.609741449356079 | 2.609741449356079 | 2.609741449356079 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-billy",
   "metadata": {},
   "source": [
    "### 0.001, relu, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "raising-president",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5620817542076111 | 0.5620817542076111 | 0.5620817542076111 |\n",
       "| pearson_corr | 0.5154631733894348 | 0.5154631733894348 | 0.5154631733894348 |\n",
       "| jensen_shannon_divergence | 1.450697898864746 | 1.450697898864746 | 1.450697898864746 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5833922028541565 | 0.5833922028541565 | 0.5833922028541565 |\n",
       "| pearson_corr | 0.5186913013458252 | 0.5186913013458252 | 0.5186913013458252 |\n",
       "| jensen_shannon_divergence | 1.5189098119735718 | 1.5189098119735718 | 1.5189098119735718 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6361128091812134 | 0.6361128091812134 | 0.6361128091812134 |\n",
       "| pearson_corr | 0.4273900091648102 | 0.4273900091648102 | 0.4273900091648102 |\n",
       "| jensen_shannon_divergence | 1.9848295450210571 | 1.9848295450210571 | 1.9848295450210571 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6070936322212219 | 0.6070936322212219 | 0.6070936322212219 |\n",
       "| pearson_corr | 0.4273258447647095 | 0.4273258447647095 | 0.4273258447647095 |\n",
       "| jensen_shannon_divergence | 1.622607946395874 | 1.622607946395874 | 1.622607946395874 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6952976584434509 | 0.6952976584434509 | 0.6952976584434509 |\n",
       "| pearson_corr | 0.27779459953308105 | 0.27779459953308105 | 0.27779459953308105 |\n",
       "| jensen_shannon_divergence | 2.3176357746124268 | 2.3176357746124268 | 2.3176357746124268 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-census",
   "metadata": {},
   "source": [
    "### 0.001, relu, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "enabling-district",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6155250072479248 | 0.6155250072479248 | 0.6155250072479248 |\n",
       "| pearson_corr | 0.4097070097923279 | 0.4097070097923279 | 0.4097070097923279 |\n",
       "| jensen_shannon_divergence | 1.7793798446655273 | 1.7793798446655273 | 1.7793798446655273 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6501258015632629 | 0.6501258015632629 | 0.6501258015632629 |\n",
       "| pearson_corr | 0.42018622159957886 | 0.42018622159957886 | 0.42018622159957886 |\n",
       "| jensen_shannon_divergence | 2.0265445709228516 | 2.0265445709228516 | 2.0265445709228516 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6055683493614197 | 0.6055683493614197 | 0.6055683493614197 |\n",
       "| pearson_corr | 0.38904237747192383 | 0.38904237747192383 | 0.38904237747192383 |\n",
       "| jensen_shannon_divergence | 1.9715359210968018 | 1.9715359210968018 | 1.9715359210968018 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7750024199485779 | 0.7750024199485779 | 0.7750024199485779 |\n",
       "| pearson_corr | 0.11765135079622269 | 0.11765135079622269 | 0.11765135079622269 |\n",
       "| jensen_shannon_divergence | 3.243908166885376 | 3.243908166885376 | 3.243908166885376 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6150321364402771 | 0.6150321364402771 | 0.6150321364402771 |\n",
       "| pearson_corr | 0.3829677402973175 | 0.3829677402973175 | 0.3829677402973175 |\n",
       "| jensen_shannon_divergence | 1.8228956460952759 | 1.8228956460952759 | 1.8228956460952759 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-right",
   "metadata": {},
   "source": [
    "### 0.001, relu, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "confident-finland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7623374462127686 | 0.7623374462127686 | 0.7623374462127686 |\n",
       "| pearson_corr | 0.3876754343509674 | 0.3876754343509674 | 0.3876754343509674 |\n",
       "| jensen_shannon_divergence | 2.9164679050445557 | 2.9164679050445557 | 2.9164679050445557 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7721250057220459 | 0.7721250057220459 | 0.7721250057220459 |\n",
       "| pearson_corr | 0.38421282172203064 | 0.38421282172203064 | 0.38421282172203064 |\n",
       "| jensen_shannon_divergence | 3.0214903354644775 | 3.0214903354644775 | 3.0214903354644775 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6586658954620361 | 0.6586658954620361 | 0.6586658954620361 |\n",
       "| pearson_corr | 0.3548034429550171 | 0.3548034429550171 | 0.3548034429550171 |\n",
       "| jensen_shannon_divergence | nan | nan | nan |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7475971579551697 | 0.7475971579551697 | 0.7475971579551697 |\n",
       "| pearson_corr | 0.36707136034965515 | 0.36707136034965515 | 0.36707136034965515 |\n",
       "| jensen_shannon_divergence | 2.712951421737671 | 2.712951421737671 | 2.712951421737671 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7480069398880005 | 0.7480069398880005 | 0.7480069398880005 |\n",
       "| pearson_corr | 0.33461683988571167 | 0.33461683988571167 | 0.33461683988571167 |\n",
       "| jensen_shannon_divergence | 2.8218843936920166 | 2.8218843936920166 | 2.8218843936920166 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-brazilian",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "### 0.01, tanh, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "proved-nitrogen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4796699285507202 | 0.4796699285507202 | 0.4796699285507202 |\n",
       "| pearson_corr | 0.6168020367622375 | 0.6168020367622375 | 0.6168020367622375 |\n",
       "| jensen_shannon_divergence | 1.2279809713363647 | 1.2279809713363647 | 1.2279809713363647 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.48440930247306824 | 0.48440930247306824 | 0.48440930247306824 |\n",
       "| pearson_corr | 0.6271598935127258 | 0.6271598935127258 | 0.6271598935127258 |\n",
       "| jensen_shannon_divergence | 1.236902117729187 | 1.236902117729187 | 1.236902117729187 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5113397240638733 | 0.5113397240638733 | 0.5113397240638733 |\n",
       "| pearson_corr | 0.6046585440635681 | 0.6046585440635681 | 0.6046585440635681 |\n",
       "| jensen_shannon_divergence | 1.4427275657653809 | 1.4427275657653809 | 1.4427275657653809 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.49554434418678284 | 0.49554434418678284 | 0.49554434418678284 |\n",
       "| pearson_corr | 0.6114315986633301 | 0.6114315986633301 | 0.6114315986633301 |\n",
       "| jensen_shannon_divergence | 1.2991265058517456 | 1.2991265058517456 | 1.2991265058517456 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47992441058158875 | 0.47992441058158875 | 0.47992441058158875 |\n",
       "| pearson_corr | 0.6218925714492798 | 0.6218925714492798 | 0.6218925714492798 |\n",
       "| jensen_shannon_divergence | 1.303899884223938 | 1.303899884223938 | 1.303899884223938 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-monroe",
   "metadata": {},
   "source": [
    "### 0.01, tanh, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "amended-driver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5890458226203918 | 0.5890458226203918 | 0.5890458226203918 |\n",
       "| pearson_corr | 0.5695328712463379 | 0.5695328712463379 | 0.5695328712463379 |\n",
       "| jensen_shannon_divergence | 1.5311962366104126 | 1.5311962366104126 | 1.5311962366104126 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6162843704223633 | 0.6162843704223633 | 0.6162843704223633 |\n",
       "| pearson_corr | 0.559210479259491 | 0.559210479259491 | 0.559210479259491 |\n",
       "| jensen_shannon_divergence | 1.6688910722732544 | 1.6688910722732544 | 1.6688910722732544 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5523518323898315 | 0.5523518323898315 | 0.5523518323898315 |\n",
       "| pearson_corr | 0.5504388809204102 | 0.5504388809204102 | 0.5504388809204102 |\n",
       "| jensen_shannon_divergence | 1.6516025066375732 | 1.6516025066375732 | 1.6516025066375732 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6141529679298401 | 0.6141529679298401 | 0.6141529679298401 |\n",
       "| pearson_corr | 0.42393481731414795 | 0.42393481731414795 | 0.42393481731414795 |\n",
       "| jensen_shannon_divergence | 1.6397494077682495 | 1.6397494077682495 | 1.6397494077682495 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6460863947868347 | 0.6460863947868347 | 0.6460863947868347 |\n",
       "| pearson_corr | 0.4933033883571625 | 0.4933033883571625 | 0.4933033883571625 |\n",
       "| jensen_shannon_divergence | 1.883154034614563 | 1.883154034614563 | 1.883154034614563 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-conservative",
   "metadata": {},
   "source": [
    "### 0.01, tanh, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "incorrect-deadline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5421869158744812 | 0.5421869158744812 | 0.5421869158744812 |\n",
       "| pearson_corr | 0.5870429277420044 | 0.5870429277420044 | 0.5870429277420044 |\n",
       "| jensen_shannon_divergence | 1.3500149250030518 | 1.3500149250030518 | 1.3500149250030518 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5680199861526489 | 0.5680199861526489 | 0.5680199861526489 |\n",
       "| pearson_corr | 0.5774297118186951 | 0.5774297118186951 | 0.5774297118186951 |\n",
       "| jensen_shannon_divergence | 1.4538654088974 | 1.4538654088974 | 1.4538654088974 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5396302342414856 | 0.5396302342414856 | 0.5396302342414856 |\n",
       "| pearson_corr | 0.5817349553108215 | 0.5817349553108215 | 0.5817349553108215 |\n",
       "| jensen_shannon_divergence | 1.551326870918274 | 1.551326870918274 | 1.551326870918274 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6148779392242432 | 0.6148779392242432 | 0.6148779392242432 |\n",
       "| pearson_corr | 0.371837854385376 | 0.371837854385376 | 0.371837854385376 |\n",
       "| jensen_shannon_divergence | 1.702768325805664 | 1.702768325805664 | 1.702768325805664 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47715190052986145 | 0.47715190052986145 | 0.47715190052986145 |\n",
       "| pearson_corr | 0.6223078966140747 | 0.6223078966140747 | 0.6223078966140747 |\n",
       "| jensen_shannon_divergence | 1.2892893552780151 | 1.2892893552780151 | 1.2892893552780151 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-patrol",
   "metadata": {},
   "source": [
    "### 0.01, tanh, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "electoral-walker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5463314652442932 | 0.5463314652442932 | 0.5463314652442932 |\n",
       "| pearson_corr | 0.5762399435043335 | 0.5762399435043335 | 0.5762399435043335 |\n",
       "| jensen_shannon_divergence | 1.3517961502075195 | 1.3517961502075195 | 1.3517961502075195 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.578883945941925 | 0.578883945941925 | 0.578883945941925 |\n",
       "| pearson_corr | 0.573122501373291 | 0.573122501373291 | 0.573122501373291 |\n",
       "| jensen_shannon_divergence | 1.490646481513977 | 1.490646481513977 | 1.490646481513977 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5961581468582153 | 0.5961581468582153 | 0.5961581468582153 |\n",
       "| pearson_corr | 0.4599166512489319 | 0.4599166512489319 | 0.4599166512489319 |\n",
       "| jensen_shannon_divergence | 1.8119326829910278 | 1.8119326829910278 | 1.8119326829910278 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5978277325630188 | 0.5978277325630188 | 0.5978277325630188 |\n",
       "| pearson_corr | 0.43213799595832825 | 0.43213799595832825 | 0.43213799595832825 |\n",
       "| jensen_shannon_divergence | 1.596254825592041 | 1.596254825592041 | 1.596254825592041 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4937050938606262 | 0.4937050938606262 | 0.4937050938606262 |\n",
       "| pearson_corr | 0.5881566405296326 | 0.5881566405296326 | 0.5881566405296326 |\n",
       "| jensen_shannon_divergence | 1.3946946859359741 | 1.3946946859359741 | 1.3946946859359741 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "        net = layers.Dense(64, activation='tanh')(net)\n",
    "        net = layers.Dense(32, activation='tanh')(net)\n",
    "        net = layers.Dense(16, activation='tanh')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-software",
   "metadata": {},
   "source": [
    "### 0.01, softmax, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "certain-nebraska",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47322073578834534 | 0.47322073578834534 | 0.47322073578834534 |\n",
       "| pearson_corr | 0.6193441152572632 | 0.6193441152572632 | 0.6193441152572632 |\n",
       "| jensen_shannon_divergence | 1.201724648475647 | 1.201724648475647 | 1.201724648475647 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4550451338291168 | 0.4550451338291168 | 0.4550451338291168 |\n",
       "| pearson_corr | 0.6619402170181274 | 0.6619402170181274 | 0.6619402170181274 |\n",
       "| jensen_shannon_divergence | 1.1394068002700806 | 1.1394068002700806 | 1.1394068002700806 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.48925140500068665 | 0.48925140500068665 | 0.48925140500068665 |\n",
       "| pearson_corr | 0.6371100544929504 | 0.6371100544929504 | 0.6371100544929504 |\n",
       "| jensen_shannon_divergence | 1.3189502954483032 | 1.3189502954483032 | 1.3189502954483032 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4833774268627167 | 0.4833774268627167 | 0.4833774268627167 |\n",
       "| pearson_corr | 0.6175777912139893 | 0.6175777912139893 | 0.6175777912139893 |\n",
       "| jensen_shannon_divergence | 1.2482527494430542 | 1.2482527494430542 | 1.2482527494430542 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.48155197501182556 | 0.48155197501182556 | 0.48155197501182556 |\n",
       "| pearson_corr | 0.6152846217155457 | 0.6152846217155457 | 0.6152846217155457 |\n",
       "| jensen_shannon_divergence | 1.304391860961914 | 1.304391860961914 | 1.304391860961914 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-nothing",
   "metadata": {},
   "source": [
    "### 0.01, softmax, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "final-greene",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5884733200073242 | 0.5884733200073242 | 0.5884733200073242 |\n",
       "| pearson_corr | 0.5692443251609802 | 0.5692443251609802 | 0.5692443251609802 |\n",
       "| jensen_shannon_divergence | 1.527906894683838 | 1.527906894683838 | 1.527906894683838 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6166061758995056 | 0.6166061758995056 | 0.6166061758995056 |\n",
       "| pearson_corr | 0.5591661930084229 | 0.5591661930084229 | 0.5591661930084229 |\n",
       "| jensen_shannon_divergence | 1.6708323955535889 | 1.6708323955535889 | 1.6708323955535889 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5517273545265198 | 0.5517273545265198 | 0.5517273545265198 |\n",
       "| pearson_corr | 0.5483113527297974 | 0.5483113527297974 | 0.5483113527297974 |\n",
       "| jensen_shannon_divergence | 1.658731460571289 | 1.658731460571289 | 1.658731460571289 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6126061081886292 | 0.6126061081886292 | 0.6126061081886292 |\n",
       "| pearson_corr | 0.428195983171463 | 0.428195983171463 | 0.428195983171463 |\n",
       "| jensen_shannon_divergence | 1.6291049718856812 | 1.6291049718856812 | 1.6291049718856812 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6454538702964783 | 0.6454538702964783 | 0.6454538702964783 |\n",
       "| pearson_corr | 0.4947802424430847 | 0.4947802424430847 | 0.4947802424430847 |\n",
       "| jensen_shannon_divergence | 1.8742262125015259 | 1.8742262125015259 | 1.8742262125015259 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-pension",
   "metadata": {},
   "source": [
    "### 0.01, softmax, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "prescription-wallace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5413152575492859 | 0.5413152575492859 | 0.5413152575492859 |\n",
       "| pearson_corr | 0.5919100046157837 | 0.5919100046157837 | 0.5919100046157837 |\n",
       "| jensen_shannon_divergence | 1.3468934297561646 | 1.3468934297561646 | 1.3468934297561646 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.564606785774231 | 0.564606785774231 | 0.564606785774231 |\n",
       "| pearson_corr | 0.587258517742157 | 0.587258517742157 | 0.587258517742157 |\n",
       "| jensen_shannon_divergence | 1.4334783554077148 | 1.4334783554077148 | 1.4334783554077148 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5393616557121277 | 0.5393616557121277 | 0.5393616557121277 |\n",
       "| pearson_corr | 0.5872717499732971 | 0.5872717499732971 | 0.5872717499732971 |\n",
       "| jensen_shannon_divergence | 1.5383365154266357 | 1.5383365154266357 | 1.5383365154266357 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6149714589118958 | 0.6149714589118958 | 0.6149714589118958 |\n",
       "| pearson_corr | 0.36591705679893494 | 0.36591705679893494 | 0.36591705679893494 |\n",
       "| jensen_shannon_divergence | 1.6933759450912476 | 1.6933759450912476 | 1.6933759450912476 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.481962651014328 | 0.481962651014328 | 0.481962651014328 |\n",
       "| pearson_corr | 0.615178644657135 | 0.615178644657135 | 0.615178644657135 |\n",
       "| jensen_shannon_divergence | 1.301253080368042 | 1.301253080368042 | 1.301253080368042 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-montgomery",
   "metadata": {},
   "source": [
    "### 0.01, softmax, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "guilty-disorder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.544914186000824 | 0.544914186000824 | 0.544914186000824 |\n",
       "| pearson_corr | 0.5805689096450806 | 0.5805689096450806 | 0.5805689096450806 |\n",
       "| jensen_shannon_divergence | 1.3498083353042603 | 1.3498083353042603 | 1.3498083353042603 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5646777749061584 | 0.5646777749061584 | 0.5646777749061584 |\n",
       "| pearson_corr | 0.5787746906280518 | 0.5787746906280518 | 0.5787746906280518 |\n",
       "| jensen_shannon_divergence | 1.4258112907409668 | 1.4258112907409668 | 1.4258112907409668 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5690332055091858 | 0.5690332055091858 | 0.5690332055091858 |\n",
       "| pearson_corr | 0.5114309191703796 | 0.5114309191703796 | 0.5114309191703796 |\n",
       "| jensen_shannon_divergence | 1.753396987915039 | 1.753396987915039 | 1.753396987915039 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6144631505012512 | 0.6144631505012512 | 0.6144631505012512 |\n",
       "| pearson_corr | 0.3713941276073456 | 0.3713941276073456 | 0.3713941276073456 |\n",
       "| jensen_shannon_divergence | 1.7007490396499634 | 1.7007490396499634 | 1.7007490396499634 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.48557648062705994 | 0.48557648062705994 | 0.48557648062705994 |\n",
       "| pearson_corr | 0.6161274909973145 | 0.6161274909973145 | 0.6161274909973145 |\n",
       "| jensen_shannon_divergence | 1.3402774333953857 | 1.3402774333953857 | 1.3402774333953857 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='softmax')(in_layer)\n",
    "        net = layers.Dense(64, activation='softmax')(net)\n",
    "        net = layers.Dense(32, activation='softmax')(net)\n",
    "        net = layers.Dense(16, activation='softmax')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-stake",
   "metadata": {},
   "source": [
    "### 0.01, sigmoid, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "expressed-collins",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47132736444473267 | 0.47132736444473267 | 0.47132736444473267 |\n",
       "| pearson_corr | 0.6244432330131531 | 0.6244432330131531 | 0.6244432330131531 |\n",
       "| jensen_shannon_divergence | 1.1852149963378906 | 1.1852149963378906 | 1.1852149963378906 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4745608866214752 | 0.4745608866214752 | 0.4745608866214752 |\n",
       "| pearson_corr | 0.6308768391609192 | 0.6308768391609192 | 0.6308768391609192 |\n",
       "| jensen_shannon_divergence | 1.191916584968567 | 1.191916584968567 | 1.191916584968567 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4923296868801117 | 0.4923296868801117 | 0.4923296868801117 |\n",
       "| pearson_corr | 0.6318956613540649 | 0.6318956613540649 | 0.6318956613540649 |\n",
       "| jensen_shannon_divergence | 1.3238669633865356 | 1.3238669633865356 | 1.3238669633865356 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4788658320903778 | 0.4788658320903778 | 0.4788658320903778 |\n",
       "| pearson_corr | 0.622636079788208 | 0.622636079788208 | 0.622636079788208 |\n",
       "| jensen_shannon_divergence | 1.2237476110458374 | 1.2237476110458374 | 1.2237476110458374 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4822985529899597 | 0.4822985529899597 | 0.4822985529899597 |\n",
       "| pearson_corr | 0.6162485480308533 | 0.6162485480308533 | 0.6162485480308533 |\n",
       "| jensen_shannon_divergence | 1.2997170686721802 | 1.2997170686721802 | 1.2997170686721802 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-hierarchy",
   "metadata": {},
   "source": [
    "### 0.01, sigmoid, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "accessible-danish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5889586210250854 | 0.5889586210250854 | 0.5889586210250854 |\n",
       "| pearson_corr | 0.5696021318435669 | 0.5696021318435669 | 0.5696021318435669 |\n",
       "| jensen_shannon_divergence | 1.530744194984436 | 1.530744194984436 | 1.530744194984436 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6171060800552368 | 0.6171060800552368 | 0.6171060800552368 |\n",
       "| pearson_corr | 0.559170663356781 | 0.559170663356781 | 0.559170663356781 |\n",
       "| jensen_shannon_divergence | 1.6738697290420532 | 1.6738697290420532 | 1.6738697290420532 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5556007027626038 | 0.5556007027626038 | 0.5556007027626038 |\n",
       "| pearson_corr | 0.5410311818122864 | 0.5410311818122864 | 0.5410311818122864 |\n",
       "| jensen_shannon_divergence | 1.6791385412216187 | 1.6791385412216187 | 1.6791385412216187 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.616184651851654 | 0.616184651851654 | 0.616184651851654 |\n",
       "| pearson_corr | 0.4216943383216858 | 0.4216943383216858 | 0.4216943383216858 |\n",
       "| jensen_shannon_divergence | 1.6499992609024048 | 1.6499992609024048 | 1.6499992609024048 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6423258781433105 | 0.6423258781433105 | 0.6423258781433105 |\n",
       "| pearson_corr | 0.5023199915885925 | 0.5023199915885925 | 0.5023199915885925 |\n",
       "| jensen_shannon_divergence | 1.8540353775024414 | 1.8540353775024414 | 1.8540353775024414 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-bread",
   "metadata": {},
   "source": [
    "### 0.01, sigmoid, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "occasional-pendant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5428521633148193 | 0.5428521633148193 | 0.5428521633148193 |\n",
       "| pearson_corr | 0.5865026712417603 | 0.5865026712417603 | 0.5865026712417603 |\n",
       "| jensen_shannon_divergence | 1.3525757789611816 | 1.3525757789611816 | 1.3525757789611816 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5672164559364319 | 0.5672164559364319 | 0.5672164559364319 |\n",
       "| pearson_corr | 0.5775167346000671 | 0.5775167346000671 | 0.5775167346000671 |\n",
       "| jensen_shannon_divergence | 1.4498101472854614 | 1.4498101472854614 | 1.4498101472854614 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5395045280456543 | 0.5395045280456543 | 0.5395045280456543 |\n",
       "| pearson_corr | 0.5834408402442932 | 0.5834408402442932 | 0.5834408402442932 |\n",
       "| jensen_shannon_divergence | 1.5451713800430298 | 1.5451713800430298 | 1.5451713800430298 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6146016120910645 | 0.6146016120910645 | 0.6146016120910645 |\n",
       "| pearson_corr | 0.3716486692428589 | 0.3716486692428589 | 0.3716486692428589 |\n",
       "| jensen_shannon_divergence | 1.69893479347229 | 1.69893479347229 | 1.69893479347229 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.47766271233558655 | 0.47766271233558655 | 0.47766271233558655 |\n",
       "| pearson_corr | 0.6166882514953613 | 0.6166882514953613 | 0.6166882514953613 |\n",
       "| jensen_shannon_divergence | 1.2765625715255737 | 1.2765625715255737 | 1.2765625715255737 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-reading",
   "metadata": {},
   "source": [
    "### 0.01, sigmoid, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "outstanding-bradford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5452468395233154 | 0.5452468395233154 | 0.5452468395233154 |\n",
       "| pearson_corr | 0.5707107186317444 | 0.5707107186317444 | 0.5707107186317444 |\n",
       "| jensen_shannon_divergence | 1.3500452041625977 | 1.3500452041625977 | 1.3500452041625977 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5636408925056458 | 0.5636408925056458 | 0.5636408925056458 |\n",
       "| pearson_corr | 0.5762234926223755 | 0.5762234926223755 | 0.5762234926223755 |\n",
       "| jensen_shannon_divergence | 1.4239556789398193 | 1.4239556789398193 | 1.4239556789398193 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5983983874320984 | 0.5983983874320984 | 0.5983983874320984 |\n",
       "| pearson_corr | 0.43978390097618103 | 0.43978390097618103 | 0.43978390097618103 |\n",
       "| jensen_shannon_divergence | 1.9673043489456177 | 1.9673043489456177 | 1.9673043489456177 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.604568600654602 | 0.604568600654602 | 0.604568600654602 |\n",
       "| pearson_corr | 0.3876868188381195 | 0.3876868188381195 | 0.3876868188381195 |\n",
       "| jensen_shannon_divergence | 1.6355645656585693 | 1.6355645656585693 | 1.6355645656585693 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.4688444435596466 | 0.4688444435596466 | 0.4688444435596466 |\n",
       "| pearson_corr | 0.6375192403793335 | 0.6375192403793335 | 0.6375192403793335 |\n",
       "| jensen_shannon_divergence | 1.313736081123352 | 1.313736081123352 | 1.313736081123352 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='sigmoid')(in_layer)\n",
    "        net = layers.Dense(64, activation='sigmoid')(net)\n",
    "        net = layers.Dense(32, activation='sigmoid')(net)\n",
    "        net = layers.Dense(16, activation='sigmoid')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-plaza",
   "metadata": {},
   "source": [
    "### 0.01, relu, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "textile-cable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8274949193000793 | 0.8274949193000793 | 0.8274949193000793 |\n",
       "| pearson_corr | 0.08388651907444 | 0.08388651907444 | 0.08388651907444 |\n",
       "| jensen_shannon_divergence | 4.113559722900391 | 4.113559722900391 | 4.113559722900391 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5543349981307983 | 0.5543349981307983 | 0.5543349981307983 |\n",
       "| pearson_corr | 0.5968016982078552 | 0.5968016982078552 | 0.5968016982078552 |\n",
       "| jensen_shannon_divergence | 2.0820095539093018 | 2.0820095539093018 | 2.0820095539093018 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5859325528144836 | 0.5859325528144836 | 0.5859325528144836 |\n",
       "| pearson_corr | 0.5828325152397156 | 0.5828325152397156 | 0.5828325152397156 |\n",
       "| jensen_shannon_divergence | 2.302990436553955 | 2.302990436553955 | 2.302990436553955 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.8403235077857971 | 0.8403235077857971 | 0.8403235077857971 |\n",
       "| pearson_corr | 0.07015439122915268 | 0.07015439122915268 | 0.07015439122915268 |\n",
       "| jensen_shannon_divergence | 4.214554786682129 | 4.214554786682129 | 4.214554786682129 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6790430545806885 | 0.6790430545806885 | 0.6790430545806885 |\n",
       "| pearson_corr | 0.2616323232650757 | 0.2616323232650757 | 0.2616323232650757 |\n",
       "| jensen_shannon_divergence | 2.652376651763916 | 2.652376651763916 | 2.652376651763916 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='tanh')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-groove",
   "metadata": {},
   "source": [
    "### 0.01, relu, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "taken-brother",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.722378671169281 | 0.722378671169281 | 0.722378671169281 |\n",
       "| pearson_corr | 0.3621353805065155 | 0.3621353805065155 | 0.3621353805065155 |\n",
       "| jensen_shannon_divergence | 2.727346181869507 | 2.727346181869507 | 2.727346181869507 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.74514240026474 | 0.74514240026474 | 0.74514240026474 |\n",
       "| pearson_corr | 0.34005752205848694 | 0.34005752205848694 | 0.34005752205848694 |\n",
       "| jensen_shannon_divergence | 3.026334047317505 | 3.026334047317505 | 3.026334047317505 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6361128091812134 | 0.6361128091812134 | 0.6361128091812134 |\n",
       "| pearson_corr | 0.4273900091648102 | 0.4273900091648102 | 0.4273900091648102 |\n",
       "| jensen_shannon_divergence | 1.9848295450210571 | 1.9848295450210571 | 1.9848295450210571 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6070936322212219 | 0.6070936322212219 | 0.6070936322212219 |\n",
       "| pearson_corr | 0.4273258447647095 | 0.4273258447647095 | 0.4273258447647095 |\n",
       "| jensen_shannon_divergence | 1.622607946395874 | 1.622607946395874 | 1.622607946395874 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6687108874320984 | 0.6687108874320984 | 0.6687108874320984 |\n",
       "| pearson_corr | 0.34701934456825256 | 0.34701934456825256 | 0.34701934456825256 |\n",
       "| jensen_shannon_divergence | 2.230767011642456 | 2.230767011642456 | 2.230767011642456 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='softmax')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-qatar",
   "metadata": {},
   "source": [
    "### 0.01, relu, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "heard-wichita",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6346392035484314 | 0.6346392035484314 | 0.6346392035484314 |\n",
       "| pearson_corr | 0.4521292746067047 | 0.4521292746067047 | 0.4521292746067047 |\n",
       "| jensen_shannon_divergence | 2.186691999435425 | 2.186691999435425 | 2.186691999435425 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6623080372810364 | 0.6623080372810364 | 0.6623080372810364 |\n",
       "| pearson_corr | 0.4366540014743805 | 0.4366540014743805 | 0.4366540014743805 |\n",
       "| jensen_shannon_divergence | 2.1634976863861084 | 2.1634976863861084 | 2.1634976863861084 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.5585739612579346 | 0.5585739612579346 | 0.5585739612579346 |\n",
       "| pearson_corr | 0.5117194652557373 | 0.5117194652557373 | 0.5117194652557373 |\n",
       "| jensen_shannon_divergence | 1.795320749282837 | 1.795320749282837 | 1.795320749282837 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7316701412200928 | 0.7316701412200928 | 0.7316701412200928 |\n",
       "| pearson_corr | 0.1866425722837448 | 0.1866425722837448 | 0.1866425722837448 |\n",
       "| jensen_shannon_divergence | 2.5882906913757324 | 2.5882906913757324 | 2.5882906913757324 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6787986755371094 | 0.6787986755371094 | 0.6787986755371094 |\n",
       "| pearson_corr | 0.23445241153240204 | 0.23445241153240204 | 0.23445241153240204 |\n",
       "| jensen_shannon_divergence | 2.1594021320343018 | 2.1594021320343018 | 2.1594021320343018 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='sigmoid')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-sullivan",
   "metadata": {},
   "source": [
    "### 0.01, relu, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "hungarian-artist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7534201741218567 | 0.7534201741218567 | 0.7534201741218567 |\n",
       "| pearson_corr | 0.39772671461105347 | 0.39772671461105347 | 0.39772671461105347 |\n",
       "| jensen_shannon_divergence | 2.7222695350646973 | 2.7222695350646973 | 2.7222695350646973 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7659131288528442 | 0.7659131288528442 | 0.7659131288528442 |\n",
       "| pearson_corr | 0.3903031051158905 | 0.3903031051158905 | 0.3903031051158905 |\n",
       "| jensen_shannon_divergence | 2.8594603538513184 | 2.8594603538513184 | 2.8594603538513184 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.01, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256], 'relu', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.6339172720909119 | 0.6339172720909119 | 0.6339172720909119 |\n",
       "| pearson_corr | 0.35998985171318054 | 0.35998985171318054 | 0.35998985171318054 |\n",
       "| jensen_shannon_divergence | 1.9791148900985718 | 1.9791148900985718 | 1.9791148900985718 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 96, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'tanh']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7465409636497498 | 0.7465409636497498 | 0.7465409636497498 |\n",
       "| pearson_corr | 0.3718585968017578 | 0.3718585968017578 | 0.3718585968017578 |\n",
       "| jensen_shannon_divergence | 2.644503116607666 | 2.644503116607666 | 2.644503116607666 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "[100, 64, 0.001, <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>, 15, [512, 256, 128], 'tanh', 'relu']\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Test results \n",
       "| Metric           | Mean    | Min     | Max     |\n",
       "|:-----------------|--------:|--------:|--------:|\n",
       "| BrayCurtis | 0.7479757070541382 | 0.7479757070541382 | 0.7479757070541382 |\n",
       "| pearson_corr | 0.3475366532802582 | 0.3475366532802582 | 0.3475366532802582 |\n",
       "| jensen_shannon_divergence | 2.6964521408081055 | 2.6964521408081055 | 2.6964521408081055 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for g in combinations:\n",
    "    #entrenamos el autoencoder\n",
    "    print(g)\n",
    "    print(\"===================\\n\")\n",
    "    experiment_metrics, models, results = perform_experiment_2_mod(cv_folds=0, \n",
    "                        epochs=g[0], ##este es el que varia\n",
    "                        batch_size=g[1], \n",
    "                        learning_rate=g[2], ##este es el que varia\n",
    "                        optimizer=g[3],\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage, #--> lo quitamos porque ya lo tenemos en relativo, con el 80 no\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=g[4], \n",
    "                        layers=g[5],\n",
    "                        activation=g[6], \n",
    "                        activation_latent=g[7], \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=False,#esto se ha cambiado para que no me muestre los resultados \n",
    "                        device='/CPU:0')\n",
    "    \n",
    "    #Guardamos el modelo, encoder y decoder\n",
    "    model, encoder, _ ,decoder = models[0]\n",
    "    \n",
    "    \n",
    "    def model_fn_latent():\n",
    "        in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "        net = layers.Dense(128, activation='relu')(in_layer)\n",
    "        net = layers.Dense(64, activation='relu')(net)\n",
    "        net = layers.Dense(32, activation='relu')(net)\n",
    "        net = layers.Dense(16, activation='relu')(net)\n",
    "        out_layer = layers.Dense(latent_train.shape[1], activation='relu')(net) # 'tanh already'\n",
    "        model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    #funcion del modelado -- por ahora lo dejaremos asi, y voy a habalr con ella par aver que cambio, que tuneo\n",
    "    latent_train = encoder.predict(data_microbioma_train)\n",
    "    \n",
    "    result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                                                      latent_train,\n",
    "                                                      latent_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      data_domain_train,\n",
    "                                                      epochs=100,\n",
    "                                                      batch_size=16,\n",
    "                                                      verbose=-1)\n",
    "\n",
    "        #evaluamos el modelo\n",
    "    latent_test = encoder.predict(data_microbioma_test)\n",
    "        \n",
    "    \n",
    "        \n",
    "    predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, otu_columns, data_microbioma_test, data_domain_test)\n",
    "\n",
    "    print(\"----------------------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
