{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main capabilities Deep Latent Microbiome \n",
    "Notebook describing the main capabilities of the Deep Latent Microbiome system.\n",
    "\n",
    "Our case of study is the Maize Rhizosphere microbiome, with data from [Walters et al., 2018](https://doi.org/10.1073/pnas.1800918115).\n",
    "\n",
    "1. [Train model](#BuildModel)\n",
    "2. [Predictions Test Set](#PredictionsTestSet)\n",
    "3. [Application to novel samples](#NovelSamples)\n",
    "4. [Train model with different environmental features](#TrainOtherEnvFeatures)\n",
    "5. [Train model with different OTUs (aggregation at a higher taxonomic level)](#AggregatedData)\n",
    "6. [Transfer Learning](#TransferLearning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../Src/')\n",
    "from data import *\n",
    "from train_2 import *\n",
    "from transfer_learning import *\n",
    "from test_functions import *\n",
    "from layers import *\n",
    "from utils import *\n",
    "from loss import *\n",
    "from metric import *\n",
    "from results import *\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train reference model <a name=\"BuildModel\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read datasets\n",
    "Read OTU table and metadata table and transform to numpy objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_microbioma_train, df_microbioma_test, _, _, \\\n",
    "df_domain_train, df_domain_test, _, _, otu_columns, domain_columns = \\\n",
    "    read_df_with_transfer_learning_subset_fewerDomainFeatures(['age','Temperature','Precipitation3Days'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_microbioma_train = df_microbioma_train.to_numpy(dtype=np.float32)\n",
    "data_microbioma_test = df_microbioma_test.to_numpy(dtype=np.float32)\n",
    "data_domain_train = df_domain_train.to_numpy(dtype=np.float32)\n",
    "data_domain_test = df_domain_test.to_numpy(dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train an Autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the selected model (the best one from those with the smallest latent space (10)): no.351\n",
    "experiment_metrics, models, results = perform_experiment_2(cv_folds=0, \n",
    "                        epochs=100, \n",
    "                        batch_size=64, \n",
    "                        learning_rate=0.001, \n",
    "                        optimizer=optimizers.Adam,\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage,\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=10, \n",
    "                        layers=[512,256],\n",
    "                        activation='tanh', \n",
    "                        activation_latent='tanh', \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=True, \n",
    "                        device='/CPU:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example a OTU latent space is trained. \n",
    "To train a combined latent space: `data_domain_train=data_domain_train`\n",
    "\n",
    "When the model is being evaluated to define the hyperparameters, cv_folds=5. Once hyperparameters are selected, the autoencoder is built with the entire training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save encoders and decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, encoder, _ ,decoder = models[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder would be the second module of the predictor model (latent space --> microbial composition).\n",
    "\n",
    "The third model would be the encoder from environmental features, that does not exist in this case, because a OTU latent space was trained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model to predict latent space from environmental features \n",
    "To define the architecture of the first module of the predictor model (environmental features --> latent space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn_latent():\n",
    "    in_layer = layers.Input(shape=(data_domain_train.shape[1],))\n",
    "    net = layers.Dense(128, activation='tanh')(in_layer)\n",
    "    net = layers.Dense(64, activation='tanh')(net)\n",
    "    net = layers.Dense(32, activation='tanh')(net)\n",
    "    net = layers.Dense(16, activation='tanh')(net)\n",
    "    out_layer = layers.Dense(latent_train.shape[1], activation=None)(net) # 'tanh already'\n",
    "    model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "    model.compile(optimizer=optimizers.Adam(lr=0.001), loss=tf.keras.losses.MeanSquaredError(),\n",
    "                 metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the latent space predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_train = encoder.predict(data_microbioma_train)\n",
    "result_latent, model_latent = train_tl_noEnsemble(model_fn_latent,\n",
    "                            latent_train,\n",
    "                            latent_train,\n",
    "                            data_domain_train,\n",
    "                            data_domain_train,\n",
    "                            epochs=100,\n",
    "                            batch_size=16,\n",
    "                            verbose=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model over the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "latent_test = encoder.predict(data_microbioma_test)\n",
    "predictions = test_model_tl_noEnsemble(model_latent, decoder, Percentage, tf.keras.layers.Softmax, data_microbioma_test, data_domain_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save('../Results/Models/encoder_biome.h5')\n",
    "decoder.save('../Results/Models/decoder.h5')\n",
    "model_latent.save('../Results/Models/encoder_domain_model_latent.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_biome = encoder\n",
    "encoder_domain = model_latent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions test set <a name=\"PredictionsTestSet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save predictions in files for external use (R plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_microbioma_test.T.to_csv('../Results/otus_original_test.tsv', index=True, header=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predicted_otu_table_and_latent(pred,pred_latent,sample_names,otu_names,suffix=''):\n",
    "    df_otu = pd.DataFrame(pred, index=sample_names, columns=otu_names)\n",
    "    df_otu.T.to_csv('../Results/otus_'+suffix+'.tsv', index=True, header=True, sep='\\t')\n",
    "\n",
    "    df_latent = pd.DataFrame(pred_latent, index=sample_names)\n",
    "    df_latent.T.to_csv('../Results/latent_'+suffix+'.tsv', index=True, sep='\\t')\n",
    "    \n",
    "    return df_otu, df_latent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run prediction test set from microbiome, i.e. reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input only domain (i.e. environmental features)\n",
    "pred_latent_biome = encoder_biome.predict(data_microbioma_test)\n",
    "pred_biome = decoder.predict(pred_latent_biome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = save_predicted_otu_table_and_latent(pred_biome,pred_latent_biome,df_microbioma_test.index,df_microbioma_test.columns,'reconstAEfromBiome')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run prediction test set from domain (i.e. environmental features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input only domain (i.e. environmental features)\n",
    "pred_latent = encoder_domain.predict(data_domain_test)\n",
    "pred_domain = decoder.predict(pred_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_otu, df_pred_latent = save_predicted_otu_table_and_latent(pred_domain,pred_latent,df_microbioma_test.index,df_microbioma_test.columns,'predFromDomain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application to novel samples: prediction of novel ecosystems <a name=\"NovelSamples\"></a>\n",
    "To predict microbiome for new environmental features.\n",
    "\n",
    "To load the environmental feature values (in this case, plant age, temperature and precipitation) from a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('../Datasets/NovelEcosystems/metadata_novel_samples_only3envFeatures.csv', sep='\\t')\n",
    "metadata = metadata.set_index('X.SampleID')\n",
    "domain = metadata[['age',\n",
    "                   'Temperature',\n",
    "                   'Precipitation3Days']]\n",
    "print(domain.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_novel_samples = domain.to_numpy(dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load models (previously trained)\n",
    "* First module: encoder_domain (environmental features --> latent space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_domain = tf.keras.models.load_model('../Results/Models/encoder_domain_model_latent.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Second module: decoder (latent space --> microbial composition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = tf.keras.models.load_model('../Results/Models/decoder.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute new predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input only domain (i.e. environmental features)\n",
    "pred_latent_novel_samples = encoder_domain.predict(domain_novel_samples)\n",
    "pred_domain_novel_samples = decoder.predict(pred_latent_novel_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_otu, df_pred_latent = save_predicted_otu_table_and_latent(pred_domain_novel_samples,pred_latent_novel_samples,domain.index,df_microbioma_test.columns,'predFromDomain_novelSamples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model with different environmental features <a name=\"TrainOtherEnvFeatures\"></a>\n",
    "The only difference with the procedure described above, is to call the generic read dataset function (`read_df_with_transfer_learning_subset_fewerDomainFeatures()`) with different parameters. For example, just `['Temperature']` rather than `['age','Temperature','Precipitation3Days']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_microbioma_train, df_microbioma_test, _, _, \\\n",
    "df_domain_train, df_domain_test, _, _, otu_columns, domain_columns = \\\n",
    "    read_df_with_transfer_learning_subset_fewerDomainFeatures(['Temperature'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the steps for training the model is the the same as described above, in the [Train model](#BuildModel) section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model with different OTUs (aggregation at a higher taxonomic level) <a name=\"AggregatedData\"></a>\n",
    "The only difference with the procedure for training the model with OTU at species level, is to call a different function to read the otu table: `read_df_with_transfer_learning_subset_fewerDomainFeatures()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_microbioma_train, df_microbioma_test, _, _, \\\n",
    "df_domain_train, df_domain_test, _, _, otu_columns, domain_columns = \\\n",
    "    read_df_with_transfer_learning_subset_fewerDomainFeatures( \\\n",
    "        metadata_names=['age','Temperature','Precipitation3Days'],\n",
    "        otu_filename='../Datasets/Aggregated/otu_table_Family.csv',\n",
    "        metadata_filename='../Datasets/Aggregated/metadata_table_all_80.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OTU table should be previously aggregated at a higher taxonomic level, using the R scripts `Src/aggregating_taxonomic_levels.r`.\n",
    "The metadata file is the same.\n",
    "\n",
    "The rest of the steps for training the model is the the same as described above, in the [Train model](#BuildModel) section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning <a name=\"TransferLearning\"></a>\n",
    "This section explains how to re-use a model trained with sufficient samples (large dataset, such as [Walters et al.](https://doi.org/10.1073/pnas.1800918115) used until now) in a different but similar scenario where there are too few samples to undertake training (small dataset, such as [Maarastawi et al.](https://dx.doi.org/10.3389%2Ffmicb.2018.01295)).\n",
    "\n",
    "Here we explains the differences with the entire pipeline described above in the [Train model](#BuildModel) section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read datasets\n",
    "Read data with a call to a different functions, that allow to read two different datasets (the large and the small one), defining the environmental features to read from the metadata files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_microbioma_train, df_microbioma_test, \\\n",
    "df_microbioma_transfer_learning_train, df_microbioma_transfer_learning_test, \\\n",
    "df_domain_train, df_domain_test, df_domain_transfer_learning_train, df_domain_transfer_learning_test,\\\n",
    "otu_columns, domain_columns = \\\n",
    "    read_df_with_transfer_learning_2otufiles_differentDomainFeatures(\n",
    "              metadata_names=['age','Temperature','Precipitation3Days'],\n",
    "              otu_filename='../Datasets/Aggregated/otu_table_Phylum.csv',\n",
    "              metadata_filename='../Datasets/Aggregated/metadata_table_all_80.csv',\n",
    "              metadata_names_transfer=['pH','N', 'C', 'C.N'],\n",
    "              otu_transfer_filename='../Datasets/Maarastawi2018/otu_table_Phylum_Maarastawi2018.csv',\n",
    "              metadata_transfer_filename='../Datasets/Maarastawi2018/metadata_table_Maarastawi2018.csv')\n",
    "\n",
    "data_microbioma_transfer_learning_train = df_microbioma_transfer_learning_train.to_numpy(dtype=np.float32)\n",
    "data_microbioma_transfer_learning_test = df_microbioma_transfer_learning_test.to_numpy(dtype=np.float32)\n",
    "data_domain_transfer_learning_train = df_domain_transfer_learning_train.to_numpy(dtype=np.float32)\n",
    "data_domain_transfer_learning_test = df_domain_transfer_learning_test.to_numpy(dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that both otu files (`otu_filename` and `otu_transfer_filename`) must be defined at the same taxonomic levels, in this case, Phylum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train an Autoencoder model\n",
    "This autoencoder must be re-built, with the same code than [above](#BuildModel), but with a different datasets, with less OTUs, at Phylum level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_microbioma_train = df_microbioma_train.to_numpy(dtype=np.float32)\n",
    "experiment_metrics, models, results = perform_experiment_2(cv_folds=0, \n",
    "                        epochs=100, \n",
    "                        batch_size=64, \n",
    "                        learning_rate=0.001, \n",
    "                        optimizer=optimizers.Adam,\n",
    "                        learning_rate_scheduler=None,\n",
    "                        input_transform=Percentage,\n",
    "                        output_transform=tf.keras.layers.Softmax,\n",
    "                        reconstruction_loss=MakeLoss(LossBrayCurtis, Percentage, None), \n",
    "                        latent_space=10, \n",
    "                        layers=[512,256],\n",
    "                        activation='tanh', \n",
    "                        activation_latent='tanh', \n",
    "                        data_microbioma_train=data_microbioma_train,\n",
    "                        data_domain_train=None,\n",
    "                        show_results=True, \n",
    "                        device='/CPU:0')\n",
    "model, encoder, _ ,decoder = models[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model to predict latent space from environmental features \n",
    "In this case, this first module of the predictor model (environmental features --> latent space) must build based on the transfer learning datasets, using the variable `data_domain_transfer_learning_train` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn_tl():\n",
    "    in_layer = layers.Input(shape=(data_domain_transfer_learning_train.shape[1],))  \n",
    "    net = layers.Dense(64, activation='tanh')(in_layer)\n",
    "    net = layers.Dense(32, activation='tanh')(net)\n",
    "    net = layers.Dense(16, activation='tanh')(net)  \n",
    "    out_layer = layers.Dense(latent_transfer_learning_train.shape[1], activation=None)(net)\n",
    "    model = keras.Model(inputs=[in_layer], outputs=[out_layer], name='model')\n",
    "    model.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.keras.losses.MeanSquaredError(),\n",
    "                 metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the latent space predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_transfer_learning_train = encoder.predict(data_microbioma_transfer_learning_train)\n",
    "result_tl, model_tl = train_tl_noEnsemble(model_fn_tl,\n",
    "                            latent_transfer_learning_train,\n",
    "                            latent_transfer_learning_train,\n",
    "                            data_domain_transfer_learning_train,\n",
    "                            data_domain_transfer_learning_train,\n",
    "                            epochs=100,\n",
    "                            batch_size=16,\n",
    "                            verbose=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model over the test set\n",
    "Environmental features --> latent space --> microbial composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = test_model_tl_noEnsemble(model_tl, decoder, Percentage, tf.keras.layers.Softmax, data_microbioma_transfer_learning_test, data_domain_transfer_learning_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
